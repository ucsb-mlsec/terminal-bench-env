#!/usr/bin/env python3
import json
import os
from pathlib import Path
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import numpy as np

def normalize_embedding(embedding):
    """Normalize embedding to unit length (L2 norm = 1.0)"""
    norm = np.linalg.norm(embedding)
    if norm == 0:
        return embedding
    return embedding / norm

def main():
    # Paths
    test_images_dir = Path("/workspace/test_images")
    metadata_path = Path("/workspace/image_metadata.json")
    output_path = Path("/workspace/embeddings.json")
    
    # Load metadata
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    
    # Load CLIP model and processor
    print("Loading CLIP model...")
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    
    # Set model to evaluation mode
    model.eval()
    
    # Dictionary to store embeddings
    embeddings_data = {}
    
    # Process each image
    image_files = sorted([f for f in os.listdir(test_images_dir) if f.endswith('.png')])
    
    print(f"Processing {len(image_files)} images...")
    
    with torch.no_grad():
        for image_file in image_files:
            if image_file not in metadata:
                print(f"Warning: {image_file} not found in metadata, skipping...")
                continue
            
            print(f"Processing {image_file}...")
            
            # Load image
            image_path = test_images_dir / image_file
            image = Image.open(image_path).convert('RGB')
            
            # Get text description
            text_description = metadata[image_file]
            
            # Process image
            image_inputs = processor(images=image, return_tensors="pt")
            image_features = model.get_image_features(**image_inputs)
            image_embedding = image_features.squeeze().cpu().numpy()
            
            # Process text
            text_inputs = processor(text=[text_description], return_tensors="pt", padding=True)
            text_features = model.get_text_features(**text_inputs)
            text_embedding = text_features.squeeze().cpu().numpy()
            
            # Normalize embeddings
            image_embedding_normalized = normalize_embedding(image_embedding)
            text_embedding_normalized = normalize_embedding(text_embedding)
            
            # Store embeddings
            embeddings_data[image_file] = {
                "image_emb": image_embedding_normalized.tolist(),
                "text_emb": text_embedding_normalized.tolist()
            }
            
            # Verify normalization
            img_norm = np.linalg.norm(image_embedding_normalized)
            txt_norm = np.linalg.norm(text_embedding_normalized)
            print(f"  Image embedding norm: {img_norm:.6f}")
            print(f"  Text embedding norm: {txt_norm:.6f}")
    
    # Save embeddings to JSON
    print(f"\nSaving embeddings to {output_path}...")
    with open(output_path, 'w') as f:
        json.dump(embeddings_data, f, indent=2)
    
    print(f"Successfully generated embeddings for {len(embeddings_data)} images")
    print(f"Embedding dimensionality: {len(embeddings_data[image_files[0]]['image_emb'])}")

if __name__ == "__main__":
    main()