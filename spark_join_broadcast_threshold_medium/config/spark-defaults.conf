# Current Spark Configuration - Performance Issues Identified
# This configuration is causing suboptimal join performance
# The broadcast threshold is too low, forcing unnecessary shuffle joins

# Broadcast join threshold - currently set too low at 10MB
spark.sql.autoBroadcastJoinThreshold=10485760

# Executor memory configuration
spark.executor.memory=2g

# Number of executor instances in the cluster
spark.executor.instances=4

# Driver memory allocation
spark.driver.memory=1g

# Number of partitions for shuffle operations
spark.sql.shuffle.partitions=200

# Problem Analysis:
# - Product catalog (8MB) is below threshold - GOOD (broadcast enabled)
# - Customer dataset (45MB) exceeds threshold - causing shuffle join
# - Orders dataset (500MB) exceeds threshold - shuffle join is correct
# 
# With 2GB executor memory and 4 nodes, we have headroom to increase
# the broadcast threshold to enable broadcast join for customer dataset