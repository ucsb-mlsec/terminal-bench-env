[2024-01-15 02:15:23] INFO SparkContext: Starting Spark application: ProductionETL-Nightly
[2024-01-15 02:15:24] INFO DAGScheduler: Job 0 started: aggregate at ProductionETL.scala:145
[2024-01-15 02:15:25] INFO TaskSetManager: Starting task set 0.0 with 200 tasks
[2024-01-15 02:16:45] INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 78542 ms on executor 1 (1/200)
[2024-01-15 02:17:12] INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 85123 ms on executor 2 (2/200)
[2024-01-15 02:17:45] WARN TaskSetManager: Task 2.0 in stage 0.0 took 120345 ms while median is 82000 ms indicating potential skew
[2024-01-15 02:18:23] INFO ShuffleManager: Shuffle write for stage 0: 45.2 GB
[2024-01-15 02:18:24] WARN ShuffleManager: Shuffle spill to disk: 12.5 GB for task 5 in stage 0.0
[2024-01-15 02:19:45] ERROR TaskSetManager: Task 12 in stage 0.0 failed due to FetchFailedException: Failed to fetch shuffle block
[2024-01-15 02:19:46] INFO DAGScheduler: Resubmitting failed stage 0.0
[2024-01-15 02:20:12] WARN TaskSetManager: Task 12.1 in stage 0.0 took 185234 ms, significantly longer than median of 85000 ms
[2024-01-15 02:21:34] INFO TaskSetManager: Finished stage 0.0 after 4 retries
[2024-01-15 02:21:35] INFO DAGScheduler: Job 1 started: join at ProductionETL.scala:167
[2024-01-15 02:21:36] INFO TaskSetManager: Starting task set 1.0 with 200 tasks (static partition count)
[2024-01-15 02:21:37] WARN BroadcastManager: Broadcast join disabled for table of size 512 MB exceeding threshold of 256 MB
[2024-01-15 02:21:38] INFO ShuffleManager: Falling back to sort merge join due to broadcast size limit
[2024-01-15 02:23:45] INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 200) in 127543 ms on executor 3 (1/200)
[2024-01-15 02:24:12] WARN TaskSetManager: Task 5.0 in stage 1.0 showing high memory pressure: spill 8.2 GB to disk
[2024-01-15 02:25:34] ERROR Executor: Exception in task 15.0 in stage 1.0: java.lang.OutOfMemoryError: Java heap space
[2024-01-15 02:25:35] ERROR TaskSetManager: Task 15 in stage 1.0 failed: OutOfMemoryError during shuffle read
[2024-01-15 02:25:36] INFO DAGScheduler: Marking stage 1.0 as failed due to task failures
[2024-01-15 02:25:37] ERROR DAGScheduler: Job 1 failed: OutOfMemoryError in join operation at ProductionETL.scala:167
[2024-01-15 02:25:38] INFO SparkContext: Application attempt 1 failed, restarting
[2024-01-15 02:30:15] INFO SparkContext: Starting Spark application: ProductionETL-Nightly (attempt 2)
[2024-01-15 02:30:16] INFO DAGScheduler: Job 0 started: aggregate at ProductionETL.scala:145
[2024-01-15 02:30:17] INFO TaskSetManager: Starting task set 0.0 with 200 tasks
[2024-01-15 02:31:45] INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 88234 ms on executor 1 (1/200)
[2024-01-15 02:32:23] WARN ShuffleManager: Processing 200 shuffle partitions with highly uneven data distribution
[2024-01-15 02:32:24] WARN ShuffleManager: Partition 45 size: 8.5 GB, Partition 46 size: 125 MB - high skew detected
[2024-01-15 02:33:45] INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 215678 ms on executor 2 (46/200)
[2024-01-15 02:33:46] WARN TaskSetManager: Task 45 took 215678 ms while median is 90000 ms - data skew issue
[2024-01-15 02:34:12] INFO ShuffleManager: Total shuffle write: 52.3 GB across 200 partitions
[2024-01-15 02:34:13] WARN ShuffleManager: Partition size variance: min=45 MB, max=8.5 GB, median=250 MB
[2024-01-15 02:35:23] INFO TaskSetManager: Finished stage 0.0 with 200 completed tasks
[2024-01-15 02:35:24] INFO DAGScheduler: Job 1 started: join at ProductionETL.scala:167
[2024-01-15 02:35:25] INFO TaskSetManager: Starting task set 1.0 with 200 tasks
[2024-01-15 02:35:26] WARN BroadcastManager: Table size 485 MB close to broadcast threshold 256 MB, using sort merge join
[2024-01-15 02:37:45] INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 200) in 139234 ms on executor 1 (1/200)
[2024-01-15 02:38:23] WARN TaskSetManager: Task 8.0 in stage 1.0 spilled 6.8 GB to disk due to memory pressure
[2024-01-15 02:39:45] ERROR Executor: Exception in task 23.0 in stage 1.0: java.lang.OutOfMemoryError: GC overhead limit exceeded
[2024-01-15 02:39:46] ERROR TaskSetManager: Task 23 in stage 1.0 failed: OutOfMemoryError during shuffle operation
[2024-01-15 02:39:47] INFO DAGScheduler: Resubmitting failed task 23 in stage 1.0 (attempt 1 of 4)
[2024-01-15 02:41:12] ERROR Executor: Exception in task 23.1 in stage 1.0: java.lang.OutOfMemoryError: Java heap space
[2024-01-15 02:41:13] ERROR TaskSetManager: Task 23 in stage 1.0 failed again: OutOfMemoryError (attempt 2 of 4)
[2024-01-15 02:41:14] INFO DAGScheduler: Resubmitting failed task 23 in stage 1.0 (attempt 2 of 4)
[2024-01-15 02:42:34] WARN TaskSetManager: Task 67.0 in stage 1.0 took 420345 ms, extreme skew detected
[2024-01-15 02:43:56] ERROR Executor: Exception in task 23.2 in stage 1.0: FetchFailedException: Failed to fetch shuffle block
[2024-01-15 02:43:57] ERROR TaskSetManager: Task 23 in stage 1.0 failed: FetchFailedException (attempt 3 of 4)
[2024-01-15 02:43:58] INFO DAGScheduler: Marking stage 1.0 as failed after 3 task failures
[2024-01-15 02:43:59] ERROR DAGScheduler: Job 1 failed: Task failures in join operation at ProductionETL.scala:167
[2024-01-15 02:44:00] INFO SparkContext: Application attempt 2 failed
[2024-01-15 02:50:45] INFO SparkContext: Starting Spark application: ProductionETL-Nightly (attempt 3)
[2024-01-15 02:50:46] INFO DAGScheduler: Job 0 started: aggregate at ProductionETL.scala:145
[2024-01-15 02:50:47] INFO TaskSetManager: Starting task set 0.0 with 200 tasks (static configuration)
[2024-01-15 02:52:15] INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 87654 ms on executor 1 (1/200)
[2024-01-15 02:53:23] WARN ShuffleManager: Static partition count of 200 not optimal for current data distribution
[2024-01-15 02:54:45] INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 93421 ms on executor 2 (13/200)
[2024-01-15 02:55:34] WARN TaskSetManager: Large variance in task completion times: min=85s, max=425s, median=92s
[2024-01-15 02:56:23] INFO ShuffleManager: Shuffle write metrics: total=48.7 GB, spill=15.2 GB
[2024-01-15 02:57:12] WARN ShuffleManager: High spill ratio detected: 31% of shuffle data spilled to disk
[2024-01-15 02:58:45] INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 487234 ms on executor 3 (90/200)
[2024-01-15 02:58:46] WARN TaskSetManager: Task 89 took 487234 ms while median is 92000 ms - severe data skew
[2024-01-15 03:00:12] INFO ShuffleManager: Partition 89 processed 9.2 GB while average partition size is 245 MB
[2024-01-15 03:01:34] INFO TaskSetManager: Finished stage 0.0 after processing 200 tasks
[2024-01-15 03:01:35] INFO DAGScheduler: Job 1 started: join at ProductionETL.scala:167
[2024-01-15 03:01:36] INFO TaskSetManager: Starting task set 1.0 with 200 tasks
[2024-01-15 03:01:37] INFO BroadcastManager: Evaluating broadcast join for table of estimated size 512 MB
[2024-01-15 03:01:38] WARN BroadcastManager: Cannot use broadcast join: size exceeds spark.sql.autoBroadcastJoinThreshold=256 MB
[2024-01-15 03:01:39] INFO ShuffleManager: Initiating sort merge join with 200 partitions per side
[2024-01-15 03:03:45] INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 200) in 126543 ms on executor 1 (1/200)
[2024-01-15 03:04:23] WARN TaskSetManager: Task 3.0 in stage 1.0 spilled 7.5 GB to disk
[2024-01-15 03:05:12] INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 203) in 209876 ms on executor 2 (4/200)
[2024-01-15 03:06:34] ERROR Executor: Exception in task 18.0 in stage 1.0: java.lang.OutOfMemoryError: Java heap space during join
[2024-01-15 03:06:35] ERROR TaskSetManager: Task 18 in stage 1.0 failed: OutOfMemoryError in hash join build phase
[2024-01-15 03:06:36] INFO DAGScheduler: Resubmitting failed task 18 in stage 1.0
[2024-01-15 03:08:12] WARN TaskSetManager: Task 18.1 in stage 1.0 showing same memory pressure pattern
[2024-01-15 03:09:23] ERROR Executor: Exception in task 18.1 in stage 1.0: java.lang.OutOfMemoryError: GC overhead limit exceeded
[2024-01-15 03:09:24] ERROR TaskSetManager: Task 18 in stage 1.0 failed again (attempt 2 of 4)
[2024-01-15 03:09:25] INFO DAGScheduler: Resubmitting failed task 18 in stage 1.0
[2024-01-15 03:10:45] WARN ShuffleManager: Join operation experiencing high shuffle read: 38.5 GB
[2024-01-15 03:11:23] INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 225) in 567234 ms on executor 3 (26/200)
[2024-01-15 03:11:24] WARN TaskSetManager: Task 25 took 567234 ms, extreme outlier indicating partition skew
[2024-01-15 03:12:34] ERROR Executor: Exception in task 18.2 in stage 1.0: FetchFailedException during shuffle read
[2024-01-15 03:12:35] ERROR TaskSetManager: Task 18 failed with FetchFailedException (attempt 3 of 4)
[2024-01-15 03:12:36] ERROR DAGScheduler: Stage 1.0 failed after multiple task attempts
[2024-01-15 03:12:37] ERROR DAGScheduler: Job 1 failed: Multiple task failures in stage 1.0
[2024-01-15 03:12:38] ERROR SparkContext: Application failed after 3 attempts due to persistent OOM and skew issues
[2024-01-15 03:12:39] INFO SparkContext: Final shuffle metrics - Total read: 142.5 GB, Total write: 156.3 GB, Total spill: 45.8 GB
[2024-01-15 03:12:40] ERROR SparkContext: Root cause analysis: Data skew and static partitioning causing uneven resource utilization
[2024-01-15 03:12:41] ERROR SparkContext: Recommendation: Enable Adaptive Query Execution for runtime optimization
[2024-01-15 03:12:42] INFO SparkContext: Application ProductionETL-Nightly terminated with FAILED status