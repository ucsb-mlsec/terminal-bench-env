# CUDA Memory Allocation API Reference
# Version 11.x - Pinned Memory Types and Performance Characteristics

================================================================================
SECTION 1: CUDA PINNED MEMORY ALLOCATION TYPES
================================================================================

1.1 STANDARD PINNED MEMORY (cudaHostAllocDefault)
--------------------------------------------------

API Call:
  cudaHostAlloc(&ptr, size, cudaHostAllocDefault);

Memory Characteristics:
- CPU cacheable memory
- Full cache hierarchy support (L1, L2, L3 caches)
- Cache line size: 64 bytes (x86_64 architecture)
- Supports all memory operations: read, write, read-modify-write
- Cache coherency automatically maintained by hardware
- Optimal for patterns with good spatial/temporal locality

Performance Characteristics:
- Cache hit latency: 4-10 CPU cycles
- Cache miss latency: 100-300 CPU cycles  
- Read bandwidth: Up to 60 GB/s (cached)
- Write bandwidth: Up to 40 GB/s (with cache allocation overhead)
- Read-modify-write: Efficient when data is cache-resident

Best Use Cases:
- Patterns with temporal locality (repeated access to same addresses)
- Patterns with spatial locality (sequential or nearby addresses)
- Workloads requiring read operations
- Read-modify-write operations
- Small to medium buffer sizes where caching is beneficial

1.2 WRITE-COMBINING PINNED MEMORY (cudaHostAllocWriteCombined)
---------------------------------------------------------------

API Call:
  cudaHostAlloc(&ptr, size, cudaHostAllocWriteCombined);

Memory Characteristics:
- CPU NON-cacheable memory
- Write operations use CPU write-combining buffers
- Writes bypass all cache levels
- Write-combining buffer size: 64-128 bytes per buffer
- Multiple write-combining buffers available (CPU dependent)
- Buffers flush to memory when full or on explicit fence operations

Write-Combining Mechanism:
- Small writes accumulate in write-combining buffers
- Buffer coalesces multiple writes into larger memory transactions
- Reduces memory bus traffic and improves bandwidth
- Optimal when sequential writes fill entire combining buffer

CRITICAL LIMITATION - READ OPERATIONS:
- Read operations are UNCACHED
- Read latency: 200-1000+ CPU cycles (10-100x slower than cached reads)
- Read bandwidth: 1-5 GB/s (severely degraded)
- No speculative prefetching
- No cache line reuse
- NEVER use write-combining memory for patterns with read operations

================================================================================
SECTION 2: WRITE-COMBINING PERFORMANCE BENEFITS
================================================================================

2.1 OPTIMAL PATTERNS FOR WRITE-COMBINING
-----------------------------------------

Sequential Write Pattern:
- Access pattern: address[0], address[1], address[2], ..., address[n]
- Performance gain: +30% to +50% bandwidth improvement
- Reason: Consecutive writes fill write-combining buffers efficiently
- Write-combining buffer utilization: 90-100%
- Example: Filling image buffer line-by-line, pixel-by-pixel

Forward-Strided Pattern (small stride):
- Access pattern: address[0], address[4], address[8], ... (stride <= 64 bytes)
- Performance gain: +15% to +30% bandwidth improvement  
- Reason: Writes still land within same/adjacent combining buffers
- Write-combining buffer utilization: 60-80%

Burst Write Pattern:
- Access pattern: Write multiple consecutive elements, skip, repeat
- Performance gain: +20% to +40% for the burst regions
- Reason: Each burst fills combining buffers efficiently

2.2 BANDWIDTH IMPROVEMENTS
---------------------------

Standard Pinned Memory Sequential Write:
- Effective bandwidth: ~30-40 GB/s
- Overhead: Cache allocation, cache line fills
- Memory transactions: Many small writes to cache, periodic writebacks

Write-Combining Pinned Memory Sequential Write:
- Effective bandwidth: ~45-60 GB/s  
- Overhead: Minimal (no cache involvement)
- Memory transactions: Coalesced large writes directly to memory

Improvement Factor:
- Sequential: 1.3x to 1.5x bandwidth increase
- Large sequential buffers (> 10 MB): Up to 1.6x improvement

2.3 REQUIREMENTS FOR BENEFITS
------------------------------

MANDATORY Requirements:
1. Write-only access pattern (no reads)
2. Sequential or nearly-sequential writes
3. No read-modify-write operations
4. No verification reads

RECOMMENDED Requirements:
1. Buffer alignment: 256 bytes or greater
2. Write alignment: 4-byte, 8-byte, or 16-byte boundaries
3. Buffer size: > 1 MB (larger is better)
4. Continuous write operations without interruption

================================================================================
SECTION 3: WRITE-COMBINING PERFORMANCE PENALTIES
================================================================================

3.1 PATTERNS THAT SUFFER WITH WRITE-COMBINING
----------------------------------------------

Random Access Pattern:
- Impact: -60% to -80% performance degradation
- Reason: Random writes cannot utilize write-combining efficiently
- Reason: No caching means no locality benefits
- Write-combining buffer utilization: < 20%
- Memory transaction efficiency: Very poor

Strided Pattern (stride > 64 bytes):
- Impact: -40% to -70% performance degradation
- Reason: Stride causes writes to miss combining buffer coalescing
- Reason: Each write may flush buffer prematurely
- Write-combining buffer utilization: < 30%

Read-Heavy Pattern:
- Impact: -70% to -95% performance degradation  
- Reason: Uncached reads are 10-100x slower than cached reads
- Read operations dominate execution time
- Even small percentage of reads can destroy performance

Read-Modify-Write Pattern:
- Impact: -80% to -95% performance degradation
- Reason: Combines slow uncached read with uncached write
- No write-combining benefit due to data dependency
- Example: buffer[i] = buffer[i] + value

3.2 DETAILED PENALTY ANALYSIS
------------------------------

Read Operation Penalty:
- Cached read latency: ~10 cycles
- Uncached read latency: ~500 cycles  
- Penalty factor: 50x slower
- Impact: Even 5% reads can reduce overall performance by 50%+

Random Write Penalty:
- Write-combining efficiency: Near zero
- Lost caching benefit: Cache would have provided ~30% performance
- Net effect: -60% performance vs standard pinned memory

Large Stride Penalty (stride > 128 bytes):
- Write-combining buffers cannot coalesce writes
- Each write may cause buffer flush
- No spatial locality benefit
- Performance: Similar to or worse than standard pinned memory

================================================================================
SECTION 4: ALIGNMENT AND SIZE REQUIREMENTS
================================================================================

4.1 MEMORY ALIGNMENT
--------------------

Write-Combining Memory Allocation:
- Returned pointer alignment: 256 bytes (guaranteed by CUDA runtime)
- Recommended buffer size alignment: 4096 bytes (page size)
- Internal structure alignment: 256 bytes or greater

Access Alignment for Optimal Performance:
- 4-byte aligned writes: Good write-combining efficiency
- 8-byte aligned writes: Better write-combining efficiency  
- 16-byte aligned writes: Optimal write-combining efficiency
- Unaligned writes: Reduced efficiency, possible splitting across buffers

Impact of Misalignment:
- Unaligned sequential writes: -15% to -30% performance reduction
- Straddling write-combining buffer boundaries: Reduced coalescing

4.2 BUFFER SIZE CONSIDERATIONS
-------------------------------

Minimum Effective Size:
- Write-combining benefits appear: > 256 KB
- Significant benefits: > 1 MB
- Optimal benefits: > 10 MB

Small Buffer Penalty:
- Buffers < 256 KB: Setup overhead dominates, minimal benefit
- Buffers < 64 KB: Likely performance loss vs standard pinned

Large Buffer Advantage:
- Buffers > 10 MB: Amortized overhead, maximum bandwidth utilization
- Buffers > 100 MB: Sustained peak bandwidth throughout transfer

================================================================================
SECTION 5: CACHE BEHAVIOR COMPARISON
================================================================================

5.1 STANDARD PINNED MEMORY CACHE BEHAVIOR
------------------------------------------

Cache Hierarchy:
- L1 Cache: 32-64 KB per core, ~4 cycle latency
- L2 Cache: 256 KB - 1 MB per core, ~12 cycle latency
- L3 Cache: 8-64 MB shared, ~40 cycle latency
- Main Memory: ~200 cycle latency

Write Operation in Cached Memory:
1. Check if cache line present (cache hit/miss)
2. If miss: Allocate cache line, read from memory
3. Write data to cache line
4. Mark cache line as dirty
5. Eventually writeback to memory

Cache Line Behavior:
- Cache line size: 64 bytes
- Write allocate policy: Writing allocates cache line
- Sequential writes: Good cache line utilization
- Random writes: Poor cache line utilization, thrashing

Benefits for Different Patterns:
- Sequential: Cache line prefetching, good utilization
- Strided (stride < 64): Multiple writes share cache line
- Random: Cache provides some benefit from temporal locality

5.2 WRITE-COMBINING MEMORY CACHE BEHAVIOR
------------------------------------------

Cache Interaction:
- NONE - Write-combining memory bypasses all caches
- No cache allocation on write
- No cache lookup on read
- No cache coherency traffic

Write-Combining Buffer Behavior:
- Buffer size: 64-128 bytes (CPU implementation dependent)
- Number of buffers: Typically 4-8 buffers
- Buffer assignment: Based on memory address alignment
- Buffer flush conditions:
  * Buffer becomes full
  * Memory fence instruction (SFENCE)
  * Write to different buffer page
  * Explicit flush by software

Sequential Write Behavior:
1. Write sent to write-combining buffer based on address
2. Buffer accumulates consecutive writes
3. When buffer full (64-128 bytes accumulated)
4. Coalesced write transaction sent to memory
5. High efficiency: One memory transaction for many writes

Random Write Behavior:
1. Each write may target different buffer
2. Buffers fill slowly and inefficiently
3. Frequent premature buffer flushes
4. Many memory transactions: One per flush
5. Low efficiency: Similar transaction count to uncached writes

================================================================================
SECTION 6: DECISION GUIDELINES AND FLOWCHART
================================================================================

6.1 DECISION FLOWCHART
----------------------

START: Should I use write-combining memory?

Question 1: Does the pattern include ANY read operations?
├─ YES → DO NOT USE write-combining (use standard pinned memory)
└─ NO → Continue to Question 2

Question 2: Is the access pattern sequential or nearly-sequential?
├─ NO → DO NOT USE write-combining (use standard pinned memory)
└─ YES → Continue to Question 3

Question 3: Is the buffer size > 1 MB?
├─ NO → MAYBE (benefits unclear, test required)
└─ YES → Continue to Question 4

Question 4: Is memory alignment adequate (256+ byte alignment)?
├─ NO → DO NOT USE (realign buffer first)
└─ YES → USE WRITE-COMBINING (expect 30-50% improvement)

6.2 PATTERN-SPECIFIC RECOMMENDATIONS
-------------------------------------

PURE SEQUENTIAL WRITE PATTERN:
Decision: USE write-combining memory
Expected improvement: +30% to +50%
Requirements:
- No read operations
- Sequential address progression: buf[0], buf[1], buf[2], ...
- Buffer size > 1 MB
- Proper alignment (256+ bytes)

STRIDED PATTERN (stride <= 64 bytes):
Decision: MAYBE use write-combining
Expected improvement: +10% to +25%
Requirements:
- No read operations
- Small stride within cache line
- Buffer size > 5 MB

STRIDED PATTERN (stride > 64 bytes):
Decision: DO NOT use write-combining
Expected impact: -30% to -60% degradation
Reason: Poor write-combining efficiency, lost cache benefits

RANDOM ACCESS PATTERN:
Decision: DO NOT use write-combining
Expected impact: -60% to -80% degradation  
Reason: No write combining possible, no cache benefits

PATTERN WITH ANY READS:
Decision: DO NOT use write-combining
Expected impact: -70% to -95% degradation
Reason: Uncached reads are catastrophically slow

READ-MODIFY-WRITE PATTERN:
Decision: NEVER use write-combining
Expected impact: -80% to -95% degradation
Reason: Worst case - slow reads AND data dependencies prevent write combining

6.3 BUFFER SIZE GUIDELINES
---------------------------

Buffer Size < 256 KB:
- Recommendation: Use standard pinned memory
- Reason: Overhead dominates, minimal write-combining benefit

Buffer Size 256 KB - 1 MB:
- Recommendation: Test both approaches
- Write-combining may provide 10-20% benefit for sequential patterns

Buffer Size 1 MB - 10 MB:
- Recommendation: Use write-combining for sequential write patterns
- Expected benefit: 25-40% improvement

Buffer Size > 10 MB:
- Recommendation: Strongly consider write-combining for sequential writes
- Expected benefit: 35-50% improvement

================================================================================
SECTION 7: PERFORMANCE IMPACT SUMMARY TABLE
================================================================================

7.1 PERFORMANCE COMPARISON BY PATTERN
--------------------------------------

SEQUENTIAL WRITE-ONLY PATTERN:
Memory Type          | Bandwidth  | Latency    | Performance
---------------------|------------|------------|-------------
Standard Pinned      | 35 GB/s    | Normal     | Baseline
Write-Combining      | 50 GB/s    | Normal     | +43% BETTER
Recommendation: USE WRITE-COMBINING

STRIDED PATTERN (stride = 64 bytes, WRITE-ONLY):
Memory Type          | Bandwidth  | Latency    | Performance  
---------------------|------------|------------|-------------
Standard Pinned      | 30 GB/s    | Normal     | Baseline
Write-Combining      | 35 GB/s    | Normal     | +17% BETTER
Recommendation: MAYBE (test required)

STRIDED PATTERN (stride = 256 bytes, with READS):
Memory Type          | Bandwidth  | Latency    | Performance
---------------------|------------|------------|-------------
Standard Pinned      | 25 GB/s    | Normal     | Baseline  
Write-Combining      | 8 GB/s     | 10x reads  | -68% WORSE
Recommendation: DO NOT USE WRITE-COMBINING

RANDOM ACCESS PATTERN (with reads):
Memory Type          | Bandwidth  | Latency    | Performance
---------------------|------------|------------|-------------
Standard Pinned      | 20 GB/s    | Normal     | Baseline
Write-Combining      | 4 GB/s     | 50x reads  | -80% WORSE
Recommendation: NEVER USE WRITE-COMBINING

READ-MODIFY-WRITE PATTERN:
Memory Type          | Bandwidth  | Latency    | Performance
---------------------|------------|------------|-------------
Standard Pinned      | 18 GB/s    | Normal     | Baseline
Write-Combining      | 2 GB/s     | 100x reads | -89% WORSE  
Recommendation: NEVER USE WRITE-COMBINING

7.2 KEY PERFORMANCE FACTORS
----------------------------

Write-Combining BENEFITS When:
- Access pattern: Sequential writes
- Operation type: Write-only
- Buffer size: Large (> 1 MB)
- Expected gain: +30% to +50%

Write-Combining PENALTIES When:
- Any read operations present: -70% to -95%
- Random access pattern: -60% to -80%
- Large stride (> 64 bytes): -40% to -70%
- Read-modify-write: -80% to -95%

================================================================================
SECTION 8: IMPLEMENTATION GUIDELINES
================================================================================

8.1 CODE EXAMPLE - CORRECT USAGE
---------------------------------

// GOOD: Sequential write-only pattern
void* hostBuffer;
size_t bufferSize = 10 * 1024 * 1024; // 10 MB
cudaHostAlloc(&hostBuffer, bufferSize, cudaHostAllocWriteCombined);

float* data = (float*)hostBuffer;
// Write-only sequential pattern
for (int i = 0; i < bufferSize / sizeof(float); i++) {
    data[i] = computeValue(i); // Pure write, no read
}

// Transfer to GPU
cudaMemcpy(deviceBuffer, hostBuffer, bufferSize, cudaMemcpyHostToDevice);

8.2 CODE EXAMPLE - INCORRECT USAGE
-----------------------------------

// BAD: Read-modify-write pattern
void* hostBuffer;
cudaHostAlloc(&hostBuffer, bufferSize, cudaHostAllocWriteCombined);

float* data = (float*)hostBuffer;
// This pattern will be VERY slow with write-combining
for (int i = 0; i < n; i++) {
    data[i] = data[i] * 2.0f; // READ-modify-write: CATASTROPHIC with WC
}

// BAD: Random access pattern
for (int i = 0; i < n; i++) {
    int randomIndex = getRandomIndex();
    data[randomIndex] = value; // Random writes: NO benefit from WC
}

// BAD: Pattern with reads
for (int i = 0; i < n; i++) {
    data[i] = computeValue(i);
    if (data[i] > threshold) { // READ operation: VERY slow with WC
        data[i] = clamp(data[i]);
    }
}

================================================================================
END OF CUDA MEMORY ALLOCATION REFERENCE
================================================================================