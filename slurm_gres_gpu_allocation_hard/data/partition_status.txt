SLURM GPU CLUSTER STATUS REPORT
Generated: 2024-01-15 14:32:18 UTC
====================================================================

=== PARTITION: gpu-standard ===
Total Nodes: 8
Idle: 3, Allocated: 3, Mixed: 2, Down: 0
GPU Availability: 18/32 V100 GPUs free
Memory Available: 576GB across idle nodes
Queued Jobs: 15
Avg Wait Time: 2.5 hours

Configuration: V100 16GB GPUs, 192GB RAM per node, 4 GPUs per node
Status: Normal operations, moderate utilization
Notes: General purpose partition for standard deep learning workloads

=== PARTITION: gpu-high-mem ===
Total Nodes: 6
Idle: 1, Allocated: 4, Mixed: 1, Down: 0
GPU Availability: 6/24 A100 GPUs free
Memory Available: 512GB across idle nodes
Queued Jobs: 28
Avg Wait Time: 6.8 hours

Configuration: A100 40GB GPUs, 512GB RAM per node, 4 GPUs per node
Status: High demand - capacity constrained
Notes: Reserved for memory-intensive workloads. Priority queue active.
      Consider gpu-standard or gpu-compute for jobs under 24GB GPU memory.

=== PARTITION: gpu-compute ===
Total Nodes: 10
Idle: 6, Allocated: 3, Mixed: 1, Down: 0
GPU Availability: 32/40 A100 GPUs free
Memory Available: 1536GB across idle nodes
Queued Jobs: 8
Avg Wait Time: 1.2 hours

Configuration: A100 80GB GPUs, 256GB RAM per node, 4 GPUs per node
Status: Low utilization - good availability
Notes: High-compute partition optimized for large-scale training and inference.
      Best choice for multi-GPU jobs requiring 8+ GPUs.

====================================================================
DETAILED NODE STATUS (Sample)
====================================================================

Node: gpu-node05 | Partition: gpu-standard | State: idle | GPUs: 4/4 free (v100) | Memory: 192GB free
Node: gpu-node12 | Partition: gpu-standard | State: mixed | GPUs: 1/4 free (v100) | Memory: 48GB free
Node: gpu-node23 | Partition: gpu-high-mem | State: allocated | GPUs: 0/4 free (a100) | Memory: 0GB free
Node: gpu-node27 | Partition: gpu-high-mem | State: idle | GPUs: 4/4 free (a100) | Memory: 512GB free
Node: gpu-node34 | Partition: gpu-compute | State: idle | GPUs: 4/4 free (a100) | Memory: 256GB free
Node: gpu-node35 | Partition: gpu-compute | State: idle | GPUs: 4/4 free (a100) | Memory: 256GB free
Node: gpu-node38 | Partition: gpu-compute | State: mixed | GPUs: 2/4 free (a100) | Memory: 128GB free
Node: gpu-node41 | Partition: gpu-compute | State: allocated | GPUs: 0/4 free (a100) | Memory: 0GB free

====================================================================
CLUSTER-WIDE SUMMARY
====================================================================

Total Cluster Capacity:
- Nodes: 24 operational (0 down)
- GPUs: 96 total (56 free, 40 allocated)
- Memory: 5.5TB total (2.6TB free)
- Overall Utilization: 42%

GPU Breakdown by Type:
- V100 16GB: 32 units (18 free) - gpu-standard partition
- A100 40GB: 24 units (6 free) - gpu-high-mem partition  
- A100 80GB: 40 units (32 free) - gpu-compute partition

Total Queued Jobs: 51
Average Queue Wait Time: 3.5 hours (cluster-wide)

====================================================================
ALERTS & MAINTENANCE NOTICES
====================================================================

[INFO] gpu-high-mem partition experiencing high demand
       Recommendation: Route suitable jobs to gpu-compute for faster turnaround

[INFO] gpu-compute has excellent availability for large multi-GPU jobs
       Jobs requiring 8+ GPUs should target this partition

[SCHEDULED] Maintenance window: gpu-standard nodes gpu-node06,07
            Date: 2024-01-18 02:00-06:00 UTC
            Impact: 8 V100 GPUs offline (25% of gpu-standard capacity)

====================================================================
RESOURCE ALLOCATION RECOMMENDATIONS
====================================================================

1. Small jobs (1-2 GPUs, <16GB VRAM): Use gpu-standard
   - Current wait time: ~2.5 hours
   - Good availability

2. Memory-intensive jobs (>24GB VRAM per GPU): Use gpu-high-mem
   - Current wait time: ~6.8 hours (high demand)
   - Consider splitting workload if possible

3. Large training jobs (4+ GPUs): Use gpu-compute
   - Current wait time: ~1.2 hours (best availability)
   - Optimal for distributed training

4. Interactive/debugging sessions: Use gpu-standard
   - Faster queue times for quick turnaround

====================================================================
END OF REPORT
====================================================================