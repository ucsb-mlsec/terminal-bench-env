# SLURM Partition Configuration for GPU Cluster
# /etc/slurm/slurm_partitions.conf

# GRES Configuration
GresTypes=gpu

# Node definitions with GRES
NodeName=gpu-node[01-08] Gres=gpu:v100:4 RealMemory=192000 State=UNKNOWN
NodeName=gpu-node[09-12] Gres=gpu:a100:8 RealMemory=512000 State=UNKNOWN
NodeName=gpu-node[13-20] Gres=gpu:a40:2 RealMemory=256000 State=UNKNOWN

# Partition: gpu-standard
# V100 GPUs (16GB VRAM each), 4 GPUs per node, 192GB system RAM
# 8 nodes total, max 4 GPUs per job
PartitionName=gpu-standard Nodes=gpu-node[01-08] Default=YES MaxTime=48:00:00 State=UP DefaultTime=01:00:00 MaxNodes=1 PriorityTier=1 PreemptMode=OFF DefMemPerCPU=4000 MaxMemPerNode=192000 TRESBillingWeights="CPU=1.0,Mem=0.25G,GRES/gpu=2.0"

# Partition: gpu-high-mem
# A100 GPUs (40GB VRAM each), 8 GPUs per node, 512GB system RAM
# 4 nodes total, max 8 GPUs per job, higher priority
PartitionName=gpu-high-mem Nodes=gpu-node[09-12] Default=NO MaxTime=72:00:00 State=UP DefaultTime=01:00:00 MaxNodes=1 PriorityTier=2 PreemptMode=OFF DefMemPerCPU=8000 MaxMemPerNode=512000 TRESBillingWeights="CPU=1.0,Mem=0.25G,GRES/gpu=3.0" AllowGroups=gpu-users,research-priority

# Partition: gpu-compute
# A40 GPUs (48GB VRAM each), 2 GPUs per node, 256GB system RAM
# 8 nodes total, max 2 GPUs per job, optimized for compute workloads
PartitionName=gpu-compute Nodes=gpu-node[13-20] Default=NO MaxTime=24:00:00 State=UP DefaultTime=01:00:00 MaxNodes=1 PriorityTier=1 PreemptMode=OFF DefMemPerCPU=6000 MaxMemPerNode=256000 TRESBillingWeights="CPU=1.0,Mem=0.25G,GRES/gpu=2.5"

# Accounting storage enforcement
AccountingStorageEnforce=associations,limits,qos
AccountingStorageTRES=gres/gpu,gres/gpu:v100,gres/gpu:a100,gres/gpu:a40

# Priority configuration
PriorityType=priority/multifactor
PriorityWeightAge=1000
PriorityWeightFairshare=10000
PriorityWeightJobSize=1000
PriorityWeightPartition=10000
PriorityWeightQOS=10000
PriorityMaxAge=7-0

# Resource limits
MaxArraySize=1000
MaxJobCount=10000
MaxStepCount=40000
MaxTasksPerNode=128