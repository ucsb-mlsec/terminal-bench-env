ClusterName=hpc-cluster
ControlMachine=slurmctld
ControlAddr=slurmctld
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=localhost
AccountingStoragePort=6819
AccountingStoreFlags=job_comment
JobCompType=jobcomp/none
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log
GresTypes=gpu

NodeName=node[01-10] CPUs=32 RealMemory=128000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=1 State=UNKNOWN
NodeName=node[11-20] CPUs=64 RealMemory=256000 Sockets=2 CoresPerSocket=32 ThreadsPerCore=1 State=UNKNOWN
NodeName=gpu[01-04] CPUs=64 RealMemory=256000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Gres=gpu:a100:4 State=UNKNOWN
NodeName=gpu[05-08] CPUs=128 RealMemory=512000 Sockets=2 CoresPerSocket=32 ThreadsPerCore=2 Gres=gpu:a100:8 State=UNKNOWN

PartitionName=compute Nodes=node[01-20] Default=YES MaxTime=INFINITE State=UP
PartitionName=gpu Nodes=gpu[01-08] Default=NO MaxTime=INFINITE State=UP Priority=100
PartitionName=debug Nodes=node[01-02] Default=NO MaxTime=01:00:00 State=UP Priority=200