# Spark Configuration File
# This file contains default Spark configuration properties for the cluster

# Cluster configuration
spark.master                                yarn
spark.submit.deployMode                     client

# Executor configuration
spark.executor.memory                       4g
spark.executor.cores                        4
spark.executor.instances                    10
spark.executor.memoryOverhead              512m

# Driver configuration
spark.driver.memory                         2g
spark.driver.cores                          2
spark.driver.maxResultSize                  1g

# Shuffle and parallelism settings
spark.sql.shuffle.partitions                200
spark.default.parallelism                   100
spark.shuffle.service.enabled               true

# Adaptive Query Execution
spark.sql.adaptive.enabled                  true
spark.sql.adaptive.coalescePartitions.enabled   true
spark.sql.adaptive.skewJoin.enabled         true

# Serialization
spark.serializer                            org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max             512m

# Network and timeout settings
spark.network.timeout                       300s
spark.executor.heartbeatInterval            60s
spark.rpc.askTimeout                        120s

# SQL and Parquet settings
spark.sql.parquet.compression.codec         snappy
spark.sql.parquet.mergeSchema               false
spark.sql.parquet.filterPushdown            true

# Arrow configuration - CURRENTLY DISABLED for compatibility testing
# TODO: Re-enable after testing phase is complete
spark.sql.execution.arrow.pyspark.enabled   false
spark.sql.execution.arrow.enabled           false

# Dynamic allocation
spark.dynamicAllocation.enabled             true
spark.dynamicAllocation.minExecutors        2
spark.dynamicAllocation.maxExecutors        20
spark.dynamicAllocation.initialExecutors    5

# Event log configuration
spark.eventLog.enabled                      true
spark.eventLog.dir                          hdfs:///spark-logs

# UI and monitoring
spark.ui.port                               4040
spark.ui.retainedJobs                       100
spark.ui.retainedStages                     100