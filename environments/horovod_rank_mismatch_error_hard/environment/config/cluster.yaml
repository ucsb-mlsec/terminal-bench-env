# Distributed Training Cluster Configuration
# This file defines the parameters for coordinating multiple training workers

# Master node network address - all workers connect to this address
master_addr: '192.168.1.10'

# Port number for process group coordination
# All workers must use the same port to communicate with the master
master_port: 29500

# Total number of workers in the distributed training cluster
# CRITICAL: This must match the actual number of worker processes
# Current value is INCORRECT - should be 4, not 2
world_size: 2

# Communication backend for distributed operations
# Options: 'gloo' (CPU), 'nccl' (GPU), 'mpi'
backend: 'gloo'

# Initialization method for the process group
# 'env://' means parameters are read from environment variables
init_method: 'env://'

# Timeout in seconds for worker initialization
# Workers must connect within this time period
timeout: 1800

# Node-specific configurations (applied per worker)
workers:
  - rank: 0
    host: '192.168.1.10'
  - rank: 1
    host: '192.168.1.11'
  - rank: 2
    host: '192.168.1.12'
  - rank: 3
    host: '192.168.1.13'