2024-01-15 10:23:45,123 INFO org.apache.spark.SparkContext: Running Spark version 3.2.1
2024-01-15 10:23:45,234 INFO org.apache.spark.SparkContext: Submitted application: TransactionAnalyzerPipeline
2024-01-15 10:23:45,456 INFO org.apache.spark.SecurityManager: Changing view acls to: sparkuser
2024-01-15 10:23:45,457 INFO org.apache.spark.SecurityManager: Changing modify acls to: sparkuser
2024-01-15 10:23:45,458 INFO org.apache.spark.SecurityManager: Changing view acls groups to: 
2024-01-15 10:23:45,459 INFO org.apache.spark.SecurityManager: Changing modify acls groups to: 
2024-01-15 10:23:45,460 INFO org.apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sparkuser); groups with view permissions: Set(); users with modify permissions: Set(sparkuser); groups with modify permissions: Set()
2024-01-15 10:23:46,123 INFO org.apache.spark.executor.Executor: Starting executor ID driver on host 10.0.1.45
2024-01-15 10:23:46,234 INFO org.apache.spark.util.Utils: Successfully started service 'SparkUI' on port 4040
2024-01-15 10:23:46,345 INFO org.apache.spark.SparkContext: Added JAR file:/opt/spark/jars/spark-sql_2.12-3.2.1.jar at spark://10.0.1.45:7077/jars/spark-sql_2.12-3.2.1.jar with timestamp 1705315426345
2024-01-15 10:23:47,456 INFO org.apache.spark.sql.SparkSession: Spark Session initialized successfully
2024-01-15 10:23:47,567 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy: Reading parquet files from hdfs://namenode:9000/data/transactions/2024-01
2024-01-15 10:23:47,678 INFO org.apache.spark.sql.catalyst.optimizer.Optimizer: Applied optimization rules in 125 ms
2024-01-15 10:23:47,789 INFO org.apache.spark.sql.execution.WholeStageCodegenExec: Whole-stage codegen enabled for stage 1
2024-01-15 10:23:48,890 INFO org.apache.spark.scheduler.DAGScheduler: Got job 0 (processTransactions at transaction_analyzer.py:145) with 200 output partitions
2024-01-15 10:23:48,901 INFO org.apache.spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (processTransactions at transaction_analyzer.py:145)
2024-01-15 10:23:48,912 INFO org.apache.spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2024-01-15 10:23:48,923 INFO org.apache.spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
2024-01-15 10:23:48,934 INFO org.apache.spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[4] at processTransactions at transaction_analyzer.py:145), which has no missing parents
2024-01-15 10:23:49,045 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 512.3 KB, free 1945.6 MB)
2024-01-15 10:23:49,156 INFO org.apache.spark.storage.memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 156.7 KB, free 1945.4 MB)
2024-01-15 10:23:49,267 INFO org.apache.spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.45:38271 (size: 156.7 KB, free: 1945.8 MB)
2024-01-15 10:23:49,378 INFO org.apache.spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
2024-01-15 10:23:49,489 INFO org.apache.spark.scheduler.DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[4] at processTransactions at transaction_analyzer.py:145) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2024-01-15 10:23:49,590 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Adding task set 1.0 with 200 tasks
2024-01-15 10:23:49,701 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, 10.0.1.46, executor 1, partition 0, PROCESS_LOCAL, 7892 bytes)
2024-01-15 10:23:49,812 INFO org.apache.spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 1, 10.0.1.47, executor 2, partition 1, PROCESS_LOCAL, 7892 bytes)
2024-01-15 10:23:50,923 INFO org.apache.spark.sql.execution.aggregate.HashAggregateExec: Generating code for hash aggregate with 287 aggregate expressions
2024-01-15 10:23:51,034 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generated in 234 ms
2024-01-15 10:23:51,145 INFO org.apache.spark.sql.execution.window.WindowExec: Processing window functions over partition with 156 window specifications
2024-01-15 10:23:51,256 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Generating code for window functions with 156 expressions
2024-01-15 10:23:52,367 WARN org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generation for large query plans may take significant time
2024-01-15 10:23:52,478 INFO org.apache.spark.sql.execution.WholeStageCodegenExec: Compiling generated code for whole-stage codegen stage 1
2024-01-15 10:23:53,589 WARN org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Generated method size: 78234 bytes exceeds JVM limit of 65535 bytes
2024-01-15 10:23:53,690 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Attempting to split method into smaller methods
2024-01-15 10:23:54,801 ERROR org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Failed to compile generated code for stage 1
2024-01-15 10:23:54,912 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec: Whole-stage code generation failed
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 45, Column 8: Code of method "processNext_0(Lorg/apache/spark/sql/catalyst/InternalRow;)V" of class "org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1" grows beyond 64 KB
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.throwParseException(UnitCompiler.java:12234)
	at org.codehaus.janino.UnitCompiler.verifyMethodCode(UnitCompiler.java:11567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:7892)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:7456)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:234)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:456)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:567)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:456)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:234)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:89)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1485)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1562)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1559)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3965)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1451)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:678)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:756)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:147)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:48)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:156)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:734)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:379)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:131)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:130)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
2024-01-15 10:23:54,923 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec: Generated code statistics:
2024-01-15 10:23:54,934 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Total classes generated: 1
2024-01-15 10:23:54,945 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Main method size: 78234 bytes
2024-01-15 10:23:54,956 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Number of aggregate expressions: 287
2024-01-15 10:23:54,967 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Number of window expressions: 156
2024-01-15 10:23:54,978 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Number of projection expressions: 443
2024-01-15 10:23:54,989 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Schema field count: 443
2024-01-15 10:23:55,000 WARN org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Code generation failed due to method size exceeding JVM limit
2024-01-15 10:23:55,011 WARN org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Consider adjusting spark.sql.codegen.wholeStage or spark.sql.codegen.factoryMode
2024-01-15 10:23:55,022 INFO org.apache.spark.sql.execution.WholeStageCodegenExec: Falling back to interpreted mode due to codegen failure
2024-01-15 10:23:55,033 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 1.0 (TID 0)
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 0, 10.0.1.46, executor 1): org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 45, Column 8: Code of method "processNext_0(Lorg/apache/spark/sql/catalyst/InternalRow;)V" of class "org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1" grows beyond 64 KB
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.throwParseException(UnitCompiler.java:12234)
	at org.codehaus.janino.UnitCompiler.verifyMethodCode(UnitCompiler.java:11567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:7892)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:7456)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:234)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:456)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:567)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:456)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:234)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:89)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1485)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1562)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1559)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3965)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1451)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:678)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:756)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
2024-01-15 10:23:55,044 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
2024-01-15 10:23:55,055 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2024-01-15 10:23:55,066 INFO org.apache.spark.scheduler.TaskSchedulerImpl: Cancelling stage 1
2024-01-15 10:23:55,077 INFO org.apache.spark.scheduler.DAGScheduler: ShuffleMapStage 1 (processTransactions at transaction_analyzer.py:145) failed in 6.143 s due to org.apache.spark.SparkException: Job aborted due to stage failure
2024-01-15 10:23:55,088 ERROR org.apache.spark.scheduler.DAGScheduler: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 0, 10.0.1.46, executor 1): org.codehaus.commons.compiler.CompileException
2024-01-15 10:23:55,099 INFO org.apache.spark.sql.execution.ui.SQLAppStatusListener: Execution of query execution id=0 failed with exception: org.apache.spark.SparkException: Job aborted due to stage failure
2024-01-15 10:23:55,110 ERROR org.apache.spark.sql.execution.SQLExecution: Query execution failed with id=0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 0, 10.0.1.46, executor 1): org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 45, Column 8: Code of method "processNext_0(Lorg/apache/spark/sql/catalyst/InternalRow;)V" of class "org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1" grows beyond 64 KB
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:228)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
2024-01-15 10:23:55,121 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Detailed breakdown of generated code:
2024-01-15 10:23:55,132 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:   Aggregate expressions contribution: 42,156 bytes (53.9%)
2024-01-15 10:23:55,143 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:   Window function expressions contribution: 21,789 bytes (27.8%)
2024-01-15 10:23:55,154 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:   Projection expressions contribution: 14,289 bytes (18.3%)
2024-01-15 10:23:55,165 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: The query involves processing 443 schema fields
2024-01-15 10:23:55,176 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: With 287 aggregate operations and 156 window functions
2024-01-15 10:23:55,187 WARN org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Generated bytecode size (78234) exceeds JVM method size limit (65535)
2024-01-15 10:23:55,198 WARN org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: This typically occurs when:
2024-01-15 10:23:55,209 WARN org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:   1. Schema has too many fields (current: 443 fields)
2024-01-15 10:23:55,220 WARN org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:   2. Too many complex expressions in a single stage
2024-01-15 10:23:55,231 WARN org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:   3. spark.sql.codegen.maxFields threshold is exceeded
2024-01-15 10:23:55,242 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Current spark.sql.codegen.maxFields setting: 100
2024-01-15 10:23:55,253 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Actual field count in operations: 443
2024-01-15 10:23:55,264 WARN org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Recommendation: Increase spark.sql.codegen.maxFields to at least 450 to enable code splitting
2024-01-15 10:23:55,275 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec: Whole-stage codegen pipeline analysis:
2024-01-15 10:23:55,286 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Stage ID: 1
2024-01-15 10:23:55,297 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Operators in pipeline: HashAggregate -> Window -> Project -> Filter
2024-01-15 10:23:55,308 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Input schema fields: 443
2024-01-15 10:23:55,319 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Aggregate expressions: 287
2024-01-15 10:23:55,330 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Window specifications: 156
2024-01-15 10:23:55,341 ERROR org.apache.spark.sql.execution.WholeStageCodegenExec:   Generated method exceeded JVM bytecode limit
2024-01-15 10:23:55,352 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Method splitting analysis:
2024-01-15 10:23:55,363 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:   Cannot split method because field count (443) exceeds spark.sql.codegen.maxFields (100)
2024-01-15 10:23:55,374 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:   Method splitting requires field count to be within maxFields threshold
2024-01-15 10:23:55,385 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:   When field count > maxFields, codegen falls back to interpreted mode for that expression
2024-01-15 10:23:55,396 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:   This prevents automatic code splitting that would otherwise avoid 64KB limit
2024-01-15 10:23:55,407 WARN org.apache.spark.sql.execution.aggregate.HashAggregateExec: Aggregate operations failed code generation due to field limit
2024-01-15 10:23:55,418 WARN org.apache.spark.sql.execution.aggregate.HashAggregateExec: Number of aggregate expressions (287) combined with field count (443) triggers fallback
2024-01-15 10:23:55,429 WARN org.apache.spark.sql.execution.window.WindowExec: Window functions failed code generation due to field limit
2024-01-15 10:23:55,440 WARN org.apache.spark.sql.execution.window.WindowExec: Number of window expressions (156) combined with field count (443) exceeds codegen capacity
2024-01-15 10:23:55,451 ERROR org.apache.spark.SparkContext: Job failed: processTransactions at transaction_analyzer.py:145
2024-01-15 10:23:55,462 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://10.0.1.45:4040
2024-01-15 10:23:55,473 INFO org.apache.spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2024-01-15 10:23:55,484 INFO org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend: Shutting down all executors
2024-01-15 10:23:55,495 INFO org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
2024-01-15 10:23:55,506 WARN org.apache.spark.ExecutorAllocationManager: onExecutorRemoved(1) called, not attempting to add executors
2024-01-15 10:23:55,517 INFO org.apache.spark.ContextCleaner: Cleaned accumulator 1
2024-01-15 10:23:55,528 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
2024-01-15 10:23:55,539 INFO org.apache.spark.util.ShutdownHookManager: Shutdown hook called