# SLURM Configuration File for HPC Cluster
# Generated for heterogeneous GPU cluster deployment
# Cluster consists of 12 nodes with different GPU types

ClusterName=hpc_cluster
SlurmctldHost=controller

# Basic SLURM Parameters
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
MpiDefault=none
ProctrackType=proctrack/pgid
ReturnToService=2
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log

# Scheduling
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory

# Logging
SlurmctldDebug=info
SlurmdDebug=info

# Node Definitions with GPU Resources
# A100 nodes - 80GB GPU memory each
NodeName=node[01-04] Gres=gpu:a100:4 Feature=A100,highmem State=UNKNOWN CPUs=64 RealMemory=512000

# V100 nodes - 32GB GPU memory each
NodeName=node[05-08] Gres=gpu:v100:4 Feature=v100 State=UNKNOWN CPUs=48 RealMemory=384000

# T4 nodes - 16GB GPU memory each
NodeName=node[09-12] Gres=gpu:t4:4 Feature=T4,low_mem State=UNKNOWN CPUs=32 RealMemory=256000

# Partition Definitions
PartitionName=gpu_all Nodes=node[01-12] Default=YES MaxTime=INFINITE State=UP
PartitionName=a100 Nodes=node[01-04] MaxTime=INFINITE State=UP
PartitionName=v100 Nodes=node[05-08] MaxTime=INFINITE State=UP
PartitionName=t4 Nodes=node[09-12] MaxTime=INFINITE State=UP