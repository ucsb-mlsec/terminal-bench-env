# SLURM Configuration File
# HPC Cluster Configuration
# WARNING: TRES billing weights are currently incorrect and need fixing

ClusterName=hpc-cluster
SlurmctldHost=controller
AuthType=auth/munge
CryptoType=crypto/munge

# Scheduling
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory

# Process tracking
ProctrackType=proctrack/cgroup
TaskPlugin=task/cgroup

# Accounting Storage
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=localhost
AccountingStoragePort=6819
AccountingStorageEnforce=associations,limits,qos

# BROKEN TRES Billing Configuration
# Current configuration bills all resources equally - INCORRECT!
# AccountingStorageTRES=CPU=1.0,Mem=1.0G,GRES/gpu=1.0
# This causes unfair billing - GPU users undercharged, CPU users overcharged

# Logging
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log

# Timeouts
SlurmctldTimeout=300
SlurmdTimeout=300
MessageTimeout=60

# Node Definitions
NodeName=node[001-010] CPUs=32 RealMemory=128000 State=UNKNOWN
NodeName=node[011-020] CPUs=64 RealMemory=256000 Gres=gpu:4 State=UNKNOWN
NodeName=node[021-025] CPUs=48 RealMemory=192000 Gres=gpu:2,fpga:1 State=UNKNOWN

# Partition Definitions
PartitionName=standard Nodes=node[001-010] Default=YES MaxTime=7-00:00:00 State=UP
PartitionName=gpu Nodes=node[011-020] MaxTime=3-00:00:00 State=UP
PartitionName=special Nodes=node[021-025] MaxTime=2-00:00:00 State=UP

# Resource Limits
MaxJobCount=10000
MaxArraySize=5000