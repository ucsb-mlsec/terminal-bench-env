[2024-01-15 09:23:45.123] [INFO] CUDA Runtime initialized successfully
[2024-01-15 09:23:45.234] [INFO] Detected 4 GPU devices in system
[2024-01-15 09:23:45.345] [INFO] GPU 0: Tesla V100-SXM2-32GB (Device capability: 7.0)
[2024-01-15 09:23:45.456] [INFO] GPU 1: Tesla V100-SXM2-32GB (Device capability: 7.0)
[2024-01-15 09:23:45.567] [INFO] GPU 2: Tesla V100-SXM2-32GB (Device capability: 7.0)
[2024-01-15 09:23:45.678] [INFO] GPU 3: Tesla V100-SXM2-32GB (Device capability: 7.0)
[2024-01-15 09:23:46.123] [INFO] Batch processing job started - Job ID: BP-2024-0115-001
[2024-01-15 09:23:46.234] [INFO] Enabling peer access between GPU pairs
[2024-01-15 09:23:46.345] [INFO] Successfully enabled peer access: GPU 0 -> GPU 1
[2024-01-15 09:23:46.456] [INFO] Successfully enabled peer access: GPU 1 -> GPU 0
[2024-01-15 09:23:46.567] [INFO] Successfully enabled peer access: GPU 0 -> GPU 3
[2024-01-15 09:23:46.678] [INFO] Successfully enabled peer access: GPU 3 -> GPU 0
[2024-01-15 09:23:46.789] [INFO] Successfully enabled peer access: GPU 1 -> GPU 2
[2024-01-15 09:23:46.890] [INFO] Successfully enabled peer access: GPU 2 -> GPU 1
[2024-01-15 09:23:47.001] [INFO] Successfully enabled peer access: GPU 2 -> GPU 3
[2024-01-15 09:23:47.112] [INFO] Successfully enabled peer access: GPU 3 -> GPU 2
[2024-01-15 09:23:47.223] [WARNING] cudaErrorPeerAccessNotEnabled on GPU 0 -> GPU 2 (errno: 50)
[2024-01-15 09:23:47.334] [WARNING] cudaErrorPeerAccessNotEnabled on GPU 2 -> GPU 0 (errno: 50)
[2024-01-15 09:23:47.445] [WARNING] cudaErrorPeerAccessNotEnabled on GPU 1 -> GPU 3 (errno: 50)
[2024-01-15 09:23:47.556] [WARNING] cudaErrorPeerAccessNotEnabled on GPU 3 -> GPU 1 (errno: 50)
[2024-01-15 09:23:47.667] [INFO] Peer access topology initialization complete
[2024-01-15 09:23:48.123] [INFO] Starting data distribution phase across GPUs
[2024-01-15 09:23:48.234] [INFO] Thread-0: cudaMemcpyPeerAsync GPU 0 -> GPU 1 (128MB at 0x7f8a40000000)
[2024-01-15 09:23:48.456] [INFO] Thread-1: cudaMemcpyPeerAsync GPU 1 -> GPU 2 (256MB at 0x7f8b50000000)
[2024-01-15 09:23:48.678] [INFO] Thread-2: cudaMemcpyPeerAsync GPU 2 -> GPU 3 (512MB at 0x7f8c60000000)
[2024-01-15 09:23:49.123] [INFO] Stream-5: Data transfer complete GPU 0 -> GPU 1
[2024-01-15 09:23:49.345] [INFO] Stream-7: Data transfer complete GPU 1 -> GPU 2
[2024-01-15 09:23:49.567] [INFO] Stream-9: Data transfer complete GPU 2 -> GPU 3
[2024-01-15 09:24:12.234] [INFO] Processing batch 1 of 50
[2024-01-15 09:24:15.456] [INFO] Processing batch 2 of 50
[2024-01-15 09:24:18.678] [INFO] Processing batch 3 of 50
[2024-01-15 09:24:21.890] [WARNING] Thread-12: Retry required for GPU 3 -> GPU 0 memory access
[2024-01-15 09:24:22.001] [INFO] Processing batch 4 of 50
[2024-01-15 09:24:25.123] [ERROR] Thread-15: cudaErrorPeerAccessNotEnabled - GPU 2 -> GPU 0 (addr: 0x7f8a40200000)
[2024-01-15 09:24:25.234] [WARNING] Attempting fallback: staging through host memory
[2024-01-15 09:24:25.567] [INFO] Fallback successful - continuing processing
[2024-01-15 09:24:28.789] [INFO] Processing batch 5 of 50
[2024-01-15 09:24:31.012] [INFO] Processing batch 6 of 50
[2024-01-15 09:24:34.234] [INFO] Processing batch 7 of 50
[2024-01-15 09:24:37.456] [ERROR] Stream-18: cudaErrorInvalidValue on GPU 1 -> GPU 3 memory copy
[2024-01-15 09:24:37.567] [WARNING] Peer access validation failed for GPU pair 1-3
[2024-01-15 09:24:37.678] [INFO] Rerouting data through GPU 2 as intermediate
[2024-01-15 09:24:40.890] [INFO] Processing batch 8 of 50
[2024-01-15 09:24:44.123] [INFO] Processing batch 9 of 50
[2024-01-15 09:24:47.345] [INFO] Processing batch 10 of 50
[2024-01-15 09:24:50.567] [WARNING] Memory pressure detected on GPU 2 (28.5GB/32GB used)
[2024-01-15 09:24:53.789] [INFO] Processing batch 11 of 50
[2024-01-15 09:24:57.012] [ERROR] Thread-28: cudaErrorPeerAccessNotEnabled - GPU 0 -> GPU 2 (addr: 0x7f8c60400000)
[2024-01-15 09:24:57.123] [INFO] Applying host-staged transfer workaround
[2024-01-15 09:25:00.234] [INFO] Processing batch 12 of 50
[2024-01-15 09:25:03.456] [INFO] Processing batch 13 of 50
[2024-01-15 09:25:06.678] [INFO] Processing batch 14 of 50
[2024-01-15 09:25:09.890] [ERROR] Stream-34: cudaMemcpyPeerAsync failed GPU 3 -> GPU 1 (size: 1024MB, errno: 50)
[2024-01-15 09:25:10.001] [WARNING] Peer access not available for GPU 3 -> GPU 1, using D2H2D path
[2024-01-15 09:25:13.123] [INFO] Processing batch 15 of 50
[2024-01-15 09:25:16.345] [INFO] Processing batch 16 of 50
[2024-01-15 09:25:19.567] [CRITICAL] Thread-42: cudaErrorIllegalAddress - GPU 2 -> GPU 0 attempted access at 0x7f8a40800000
[2024-01-15 09:25:19.678] [CRITICAL] Peer memory access violation detected: source=GPU2, dest=GPU0
[2024-01-15 09:25:19.789] [CRITICAL] No peer access capability exists between GPU 2 and GPU 0
[2024-01-15 09:25:19.890] [FATAL] Unrecoverable error in critical data path - GPU 2 -> GPU 0
[2024-01-15 09:25:19.991] [FATAL] Host fallback disabled for this operation due to synchronization requirements
[2024-01-15 09:25:20.102] [FATAL] Job BP-2024-0115-001 terminated abnormally
[2024-01-15 09:25:20.213] [ERROR] Cleanup initiated - releasing GPU resources
[2024-01-15 09:25:20.324] [INFO] Batch processing aborted at batch 16 of 50
[2024-01-15 09:25:20.435] [INFO] Diagnostic data captured to log files
[2024-01-15 09:25:20.546] [INFO] System ready for maintenance