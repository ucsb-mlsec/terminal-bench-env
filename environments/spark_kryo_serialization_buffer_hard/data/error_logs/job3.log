2024-01-17 09:45:12 INFO SparkContext: Running Spark version 3.4.1
2024-01-17 09:45:12 INFO ResourceUtils: ==============================================================
2024-01-17 09:45:12 INFO ResourceUtils: No custom resources configured for spark.driver.
2024-01-17 09:45:12 INFO ResourceUtils: ==============================================================
2024-01-17 09:45:13 INFO SparkContext: Submitted application: LargeObjectAggregationJob
2024-01-17 09:45:13 INFO SecurityManager: Changing view acls to: spark
2024-01-17 09:45:13 INFO SecurityManager: Changing modify acls to: spark
2024-01-17 09:45:14 INFO Utils: Successfully started service 'sparkDriver' on port 42317.
2024-01-17 09:45:14 INFO SparkEnv: Registering MapOutputTracker
2024-01-17 09:45:14 INFO SparkEnv: Registering BlockManagerMaster
2024-01-17 09:45:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-01-17 09:45:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2024-01-17 09:45:15 INFO SparkEnv: Registering OutputCommitCoordinator
2024-01-17 09:45:15 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
2024-01-17 09:45:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
2024-01-17 09:45:16 INFO SparkContext: Added JAR file:/opt/spark/jars/spark-examples.jar at spark://spark-master:42317/jars/spark-examples.jar with timestamp 1705485916234
2024-01-17 09:45:17 INFO Executor: Starting executor ID driver on host 192.168.1.105
2024-01-17 09:45:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38291.
2024-01-17 09:45:17 INFO NettyBlockTransferService: Server created on 192.168.1.105:38291
2024-01-17 09:45:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-01-17 09:45:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.105, 38291, None)
2024-01-17 09:45:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.105:38291 with 2.3 GiB RAM, BlockManagerId(driver, 192.168.1.105, 38291, None)
2024-01-17 09:45:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.105, 38291, None)
2024-01-17 09:45:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.105, 38291, None)
2024-01-17 09:45:19 INFO SparkContext: Starting job: aggregate at LargeObjectAggregationJob.scala:78
2024-01-17 09:45:19 INFO DAGScheduler: Got job 0 (aggregate at LargeObjectAggregationJob.scala:78) with 24 output partitions
2024-01-17 09:45:19 INFO DAGScheduler: Final stage: ResultStage 0 (aggregate at LargeObjectAggregationJob.scala:78)
2024-01-17 09:45:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2024-01-17 09:45:19 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
2024-01-17 09:45:20 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at aggregate at LargeObjectAggregationJob.scala:78), which has no missing parents
2024-01-17 09:45:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.4 KiB, free 2.3 GiB)
2024-01-17 09:45:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 2.3 GiB)
2024-01-17 09:45:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.105:38291 (size: 12.8 KiB, free: 2.3 GiB)
2024-01-17 09:45:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
2024-01-17 09:45:21 INFO DAGScheduler: Submitting 24 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at aggregate at LargeObjectAggregationJob.scala:78) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2024-01-17 09:45:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 24 tasks resource profile 0
2024-01-17 09:45:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0) (192.168.1.106, executor 1, partition 0, PROCESS_LOCAL, 4572 bytes) taskResourceAssignments Map()
2024-01-17 09:45:21 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 1) (192.168.1.107, executor 2, partition 1, PROCESS_LOCAL, 4572 bytes) taskResourceAssignments Map()
2024-01-17 09:45:22 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 2) (192.168.1.108, executor 3, partition 2, PROCESS_LOCAL, 4572 bytes) taskResourceAssignments Map()
2024-01-17 09:45:22 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 3) (192.168.1.109, executor 4, partition 3, PROCESS_LOCAL, 4572 bytes) taskResourceAssignments Map()
2024-01-17 09:45:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.106:43829 (size: 12.8 KiB, free: 2.0 GiB)
2024-01-17 09:45:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.107:39174 (size: 12.8 KiB, free: 2.0 GiB)
2024-01-17 09:45:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 2147 ms on 192.168.1.106 (executor 1) (1/24)
2024-01-17 09:45:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.108:41562 (size: 12.8 KiB, free: 2.0 GiB)
2024-01-17 09:45:24 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 1) in 2934 ms on 192.168.1.107 (executor 2) (2/24)
2024-01-17 09:45:25 WARN TaskSetManager: Lost task 3.0 in stage 1.0 (TID 3) (192.168.1.109 executor 4): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 284762144. This can be caused by using a buffer that is too small for serialization. Minimum needed buffer size exceeded maximum allowed size of 67108864 bytes (64 MB). Object being serialized is [LargeAggregationBuffer]. To avoid this, increase spark.kryoserializer.buffer.max value.
	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:385)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:650)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1538)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:653)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 284762144
	at com.esotericsoftware.kryo.io.Output.require(Output.java:167)
	at com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)
	at com.esotericsoftware.kryo.io.Output.write(Output.java:219)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:47)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:568)
	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:378)
	... 6 more
2024-01-17 09:45:25 ERROR TaskSetManager: Task 3 in stage 1.0 failed 1 times; aborting job
2024-01-17 09:45:25 INFO TaskSchedulerImpl: Cancelling stage 1.0
2024-01-17 09:45:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1.0: Stage cancelled
2024-01-17 09:45:25 INFO TaskSchedulerImpl: Stage 1.0 was cancelled
2024-01-17 09:45:25 INFO DAGScheduler: ShuffleMapStage 1 (aggregate at LargeObjectAggregationJob.scala:78) failed in 4.872 s due to Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 3) (192.168.1.109 executor 4): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 284762144
2024-01-17 09:45:25 ERROR DAGScheduler: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 3) (192.168.1.109 executor 4): org.apache.spark.SparkException: Kryo serialization failed
2024-01-17 09:45:25 INFO SparkContext: Invoking stop() from shutdown hook
2024-01-17 09:45:25 INFO SparkUI: Stopped Spark web UI at http://192.168.1.105:4040