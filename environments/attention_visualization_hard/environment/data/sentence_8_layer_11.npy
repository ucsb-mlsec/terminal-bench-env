import numpy as np

# Set random seed for reproducibility
np.random.seed(42)

# Create attention weights for layer 11, sentence 8
# Using seq_len = 16 tokens
seq_len = 16
num_heads = 12

# Initialize attention weights array
attention_weights = np.zeros((num_heads, seq_len, seq_len), dtype=np.float32)

# Generate realistic attention patterns for each head
for head in range(num_heads):
    for token in range(seq_len):
        # Create different attention patterns for different heads
        if head == 0:
            # Head 0: Focus on immediate neighbors (low variance pattern)
            weights = np.exp(-np.abs(np.arange(seq_len) - token) * 2.0)
        elif head == 1:
            # Head 1: Focus on first token (CLS-like)
            weights = np.ones(seq_len) * 0.1
            weights[0] = 5.0
        elif head == 2:
            # Head 2: Uniform attention (very low variance pattern)
            weights = np.ones(seq_len) + np.random.normal(0, 0.05, seq_len)
        elif head == 3:
            # Head 3: Focus on self
            weights = np.ones(seq_len) * 0.2
            weights[token] = 3.0
        elif head == 4:
            # Head 4: Random sparse attention
            weights = np.random.exponential(0.5, seq_len)
        elif head == 5:
            # Head 5: Backward looking
            weights = np.zeros(seq_len)
            weights[:token+1] = np.exp(np.linspace(-2, 0, token+1))
        elif head == 6:
            # Head 6: Forward looking
            weights = np.zeros(seq_len)
            weights[token:] = np.exp(np.linspace(0, -2, seq_len-token))
        elif head == 7:
            # Head 7: Nearly uniform (low variance pattern)
            weights = np.ones(seq_len) + np.random.normal(0, 0.08, seq_len)
        elif head == 8:
            # Head 8: Distance-based decay
            weights = 1.0 / (1.0 + np.abs(np.arange(seq_len) - token))
        elif head == 9:
            # Head 9: Bimodal attention
            weights = np.random.gamma(2, 0.5, seq_len)
            weights[0] += 2.0
            weights[-1] += 2.0
        elif head == 10:
            # Head 10: Structured pattern
            weights = np.sin(np.arange(seq_len) * np.pi / 4) ** 2 + 0.5
        else:  # head == 11
            # Head 11: Context window
            start = max(0, token - 3)
            end = min(seq_len, token + 4)
            weights = np.ones(seq_len) * 0.05
            weights[start:end] = 1.0
        
        # Normalize to sum to 1 (proper attention distribution)
        weights = np.maximum(weights, 0)  # Ensure non-negative
        weights = weights / (weights.sum() + 1e-9)
        attention_weights[head, token, :] = weights

# Save the file
np.save('/tmp/workspace/data/sentence_8_layer_11.npy', attention_weights)