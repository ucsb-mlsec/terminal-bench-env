import numpy as np

# Set random seed for reproducibility
np.random.seed(42)

# Create attention weights for sentence 5, layer 9
seq_len = 15  # Choose a sequence length between 10-20

# Initialize array for 12 heads
attention_weights = np.zeros((12, seq_len, seq_len), dtype=np.float32)

# Generate realistic attention patterns for each head
for head in range(12):
    for token_idx in range(seq_len):
        # Create different attention patterns for different heads
        if head < 3:
            # Heads 0-2: More uniform attention (lower variance candidates)
            attn = np.random.uniform(0.8, 1.2, seq_len)
        elif head < 6:
            # Heads 3-5: Diagonal attention (to nearby tokens)
            attn = np.exp(-0.5 * ((np.arange(seq_len) - token_idx) ** 2) / 4.0)
            attn += np.random.uniform(0, 0.2, seq_len)
        elif head < 9:
            # Heads 6-8: Attention to beginning tokens
            attn = np.exp(-0.3 * np.arange(seq_len))
            attn += np.random.uniform(0, 0.3, seq_len)
        else:
            # Heads 9-11: More varied attention patterns
            attn = np.random.exponential(1.0, seq_len)
            attn += np.random.uniform(0, 0.5, seq_len)
        
        # Normalize to sum to 1.0 (proper attention distribution)
        attention_weights[head, token_idx, :] = attn / np.sum(attn)

# Save the file
np.save('data/sentence_5_layer_9.npy', attention_weights)