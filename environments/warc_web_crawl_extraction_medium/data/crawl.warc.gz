I'll generate a WARC file with the specified requirements and then process it to create the summary JSON file.

<warc_generator>
import gzip
import json
from collections import Counter
from urllib.parse import urlparse

# First, let's create the WARC file with realistic data
warc_content = []

# Define domains with varying frequencies
domains_distribution = [
    ("example.com", 35),
    ("test.org", 25),
    ("news.site.com", 18),
    ("blog.example.net", 15),
    ("api.service.io", 12),
    ("docs.platform.dev", 10),
    ("shop.store.com", 8),
    ("forum.community.org", 7),
    ("wiki.knowledge.net", 6),
    ("media.content.io", 5),
    ("app.webapp.com", 4),
    ("portal.enterprise.biz", 4),
    ("cdn.assets.net", 3),
    ("search.findme.com", 3),
    ("social.connect.org", 2),
    ("video.streaming.tv", 2),
    ("mail.email.service", 1),
    ("cloud.storage.io", 1)
]

# Status codes distribution (70% success, 30% errors)
status_codes_success = [200, 200, 200, 200, 201, 204, 206]
status_codes_error = [404, 301, 500, 503]

record_id = 1
total_records = 0

for domain, count in domains_distribution:
    for i in range(count):
        # 70% success rate
        if i < int(count * 0.7):
            status_code = status_codes_success[i % len(status_codes_success)]
            status_text = "OK" if status_code == 200 else ("Created" if status_code == 201 else "No Content" if status_code == 204 else "Partial Content")
        else:
            status_code = status_codes_error[i % len(status_codes_error)]
            status_text = "Not Found" if status_code == 404 else ("Moved Permanently" if status_code == 301 else "Internal Server Error" if status_code == 500 else "Service Unavailable")
        
        url = f"http://{domain}/page{i+1}"
        html_content = f"<html><head><title>Page {i+1} - {domain}</title></head><body><h1>Welcome to {domain}</h1><p>This is page {i+1}.</p></body></html>"
        
        http_response = f"""HTTP/1.1 {status_code} {status_text}
Date: Mon, 15 Jan 2024 10:{(record_id % 60):02d}:00 GMT
Server: Apache/2.4.41
Content-Type: text/html; charset=UTF-8
Content-Length: {len(html_content)}
Connection: close

{html_content}"""
        
        content_length = len(http_response)
        
        warc_record = f"""WARC/1.0
WARC-Type: response
WARC-Record-ID: <urn:uuid:12345678-1234-1234-1234-{record_id:012d}>
WARC-Target-URI: {url}
WARC-Date: 2024-01-15T10:{(record_id % 60):02d}:00Z
WARC-Payload-Digest: sha1:ABCDEF1234567890
Content-Type: application/http; msgtype=response
Content-Length: {content_length}

{http_response}

"""
        warc_content.append(warc_record)
        record_id += 1
        total_records += 1

# Add some request and metadata records for realism
for i in range(20):
    domain = domains_distribution[i % len(domains_distribution)][0]
    url = f"http://{domain}/page{i+1}"
    
    request_content = f"""GET /page{i+1} HTTP/1.1
Host: {domain}
User-Agent: Mozilla/5.0 (compatible; WebCrawler/1.0)
Accept: */*

"""
    
    warc_record = f"""WARC/1.0
WARC-Type: request
WARC-Record-ID: <urn:uuid:87654321-4321-4321-4321-{i:012d}>
WARC-Target-URI: {url}
WARC-Date: 2024-01-15T10:{(i % 60):02d}:00Z
Content-Type: application/http; msgtype=request
Content-Length: {len(request_content)}

{request_content}

"""
    warc_content.append(warc_record)

# Write compressed WARC file
with gzip.open('/data/crawl.warc.gz', 'wt', encoding='utf-8') as f:
    f.write(''.join(warc_content))

# Now process the WARC file to extract information
successful_urls = []
domain_counter = Counter()

with gzip.open('/data/crawl.warc.gz', 'rt', encoding='utf-8') as f:
    content = f.read()
    records = content.split('WARC/1.0\n')
    
    for record in records:
        if not record.strip():
            continue
            
        lines = record.split('\n')
        
        # Check if this is a response record
        is_response = False
        target_uri = None
        
        for line in lines:
            if line.startswith('WARC-Type:') and 'response' in line:
                is_response = True
            elif line.startswith('WARC-Target-URI:'):
                target_uri = line.split(':', 1)[1].strip()
        
        if is_response and target_uri:
            # Find the HTTP status line
            for line in lines:
                if line.startswith('HTTP/1.1') or line.startswith('HTTP/1.0'):
                    parts = line.split()
                    if len(parts) >= 2:
                        try:
                            status_code = int(parts[1])
                            if 200 <= status_code <= 299:
                                successful_urls.append(target_uri)
                                parsed = urlparse(target_uri)
                                domain = parsed.netloc
                                domain_counter[domain] += 1
                        except ValueError:
                            pass
                    break

# Calculate summary statistics
total_urls = len(successful_urls)
unique_domains = len(domain_counter)
top_domain = domain_counter.most_common(1)[0][0] if domain_counter else ""

# Create output JSON
summary = {
    "total_urls": total_urls,
    "unique_domains": unique_domains,
    "top_domain": top_domain
}

# Write output file
with open('/output/crawl_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)

</warc_generator>