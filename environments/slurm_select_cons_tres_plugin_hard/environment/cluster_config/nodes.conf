# SLURM Node Configuration
# Compute cluster nodes - Building 3, Floor 2
# Last updated: 2024-01-15

# High-memory GPU nodes - NVIDIA A100 equipped
NodeName=compute-01 CPUs=64 RealMemory=256000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN Gres=gpu:2

# Standard compute node - Tesla V100 GPUs
NodeName=compute-02 CPUs=48 RealMemory=128000 Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 State=UNKNOWN

# GPU development node - Mixed GPU types for testing
# Recently upgraded with additional GPU
NodeName=compute-03 CPUs=64 RealMemory=192000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN Gres=gpu:4

# CPU-only node for non-GPU workloads
NodeName=compute-04 CPUs=128 RealMemory=512000 Sockets=2 CoresPerSocket=32 ThreadsPerCore=2 State=UNKNOWN

# Node partition assignments
# compute-01,03: GPU partition
# compute-02: General partition (GPU recently removed for maintenance)
# compute-04: High-memory partition