2023-05-15 14:32:45,123 DEBUG [main] org.apache.spark.sql.hive.thriftserver.SparkSQLEnv: Initializing Spark SQL environment
2023-05-15 14:32:45,234 DEBUG [main] org.apache.spark.SparkConf: spark.master = local[*]
2023-05-15 14:32:45,235 DEBUG [main] org.apache.spark.SparkConf: spark.app.name = Thrift JDBC/ODBC Server
2023-05-15 14:32:45,456 DEBUG [main] org.apache.spark.sql.internal.SharedState: Creating shared state
2023-05-15 14:32:45,567 DEBUG [main] org.apache.hadoop.hive.conf.HiveConf: hive.metastore.uris = thrift://localhost:9083
2023-05-15 14:32:45,678 DEBUG [main] org.apache.hadoop.hive.conf.HiveConf: hive.server2.thrift.port = 10000
2023-05-15 14:32:45,789 DEBUG [main] org.apache.spark.sql.hive.HiveUtils: Initializing HiveMetastoreClient
2023-05-15 14:32:46,012 DEBUG [main] org.apache.hadoop.hive.metastore.HiveMetaStoreClient: Trying to connect to metastore on port 9083
2023-05-15 14:32:46,234 DEBUG [main] org.apache.thrift.transport.TSocket: Opening socket to localhost:9083
2023-05-15 14:32:46,345 DEBUG [main] org.apache.hadoop.hive.metastore.HiveMetaStoreClient: Connected to metastore
2023-05-15 14:32:46,456 DEBUG [main] org.apache.spark.sql.hive.client.HiveClientImpl: Retrieving database list
2023-05-15 14:32:46,567 DEBUG [main] org.apache.spark.sql.hive.client.HiveClientImpl: Found databases: [default, test_db, temp_db, sales_analytics]
2023-05-15 14:32:46,678 DEBUG [main] org.apache.hadoop.hive.conf.HiveConf: hive.server2.thrift.bind.host = 0.0.0.0
2023-05-15 14:32:46,789 DEBUG [main] org.apache.spark.sql.internal.SessionState: Setting default database to sales_analytics
2023-05-15 14:32:46,890 DEBUG [main] org.apache.spark.sql.catalyst.catalog.SessionCatalog: Current database set to: sales_analytics
2023-05-15 14:32:47,001 DEBUG [main] org.apache.hive.service.server.HiveServer2: Initializing HiveServer2
2023-05-15 14:32:47,112 DEBUG [main] org.apache.hive.service.cli.CLIService: Initializing CLI service
2023-05-15 14:32:47,223 DEBUG [main] org.apache.hive.service.cli.session.SessionManager: Creating session manager
2023-05-15 14:32:47,334 DEBUG [main] org.apache.hive.service.cli.session.SessionManager: Session timeout = 3600000 ms
2023-05-15 14:32:47,445 DEBUG [main] org.apache.hive.service.cli.operation.OperationManager: Initializing operation manager
2023-05-15 14:32:47,556 DEBUG [main] org.apache.hadoop.hive.conf.HiveConf: hive.server2.enable.doAs = true
2023-05-15 14:32:47,667 DEBUG [main] org.apache.hadoop.hive.conf.HiveConf: hive.server2.transport.mode = binary
2023-05-15 14:32:47,778 DEBUG [main] org.apache.hive.service.auth.HiveAuthFactory: Initializing authentication factory
2023-05-15 14:32:47,889 DEBUG [main] org.apache.hive.service.auth.HiveAuthFactory: Authentication type: NONE
2023-05-15 14:32:48,001 DEBUG [main] org.apache.hive.service.cli.thrift.ThriftCLIService: Initializing Thrift CLI service
2023-05-15 14:32:48,112 DEBUG [main] org.apache.hive.service.cli.thrift.ThriftBinaryCLIService: Starting Thrift binary CLI service
2023-05-15 14:32:48,223 DEBUG [main] org.apache.thrift.server.TThreadPoolServer: Creating thread pool with min=5, max=500 threads
2023-05-15 14:32:48,334 DEBUG [main] org.apache.thrift.transport.TServerSocket: Creating server socket on port 10000
2023-05-15 14:32:48,445 INFO  [main] org.apache.hive.service.cli.thrift.ThriftBinaryCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads
2023-05-15 14:32:48,556 DEBUG [main] org.apache.thrift.transport.TServerSocket: ServerSocket created on port 10000
2023-05-15 14:32:48,667 DEBUG [main] org.apache.hive.service.server.HiveServer2: Thrift server started successfully
2023-05-15 14:32:48,778 DEBUG [main] org.apache.spark.sql.hive.thriftserver.HiveThriftServer2: Web UI enabled on port 8080
2023-05-15 14:32:48,889 DEBUG [Thread-1] org.apache.thrift.server.TThreadPoolServer: Server thread started, waiting for connections
2023-05-15 14:32:49,001 DEBUG [main] org.apache.spark.sql.SparkSession: Active Spark session exists for default database
2023-05-15 14:32:49,112 DEBUG [main] org.apache.hadoop.hive.ql.metadata.Hive: Loading tables from database: sales_analytics
2023-05-15 14:32:49,223 DEBUG [pool-1-thread-1] org.apache.spark.sql.execution.ui.SQLListener: SQL execution listener initialized
2023-05-15 14:32:49,334 DEBUG [pool-1-thread-2] org.apache.spark.memory.MemoryManager: Total memory: 2048 MB, execution memory: 1024 MB
2023-05-15 14:33:12,445 DEBUG [Thread-5] org.apache.thrift.server.TThreadPoolServer: New connection from client 192.168.1.105
2023-05-15 14:33:12,556 DEBUG [Thread-5] org.apache.thrift.transport.TSocket: Client connected from 192.168.1.105:51234
2023-05-15 14:33:12,667 DEBUG [pool-2-thread-3] org.apache.hive.service.cli.session.SessionManager: Creating new session for user: analyst1
2023-05-15 14:33:12,778 DEBUG [pool-2-thread-3] org.apache.hive.service.cli.session.HiveSessionImpl: Session created: SessionHandle [12345-abcd-6789]
2023-05-15 14:33:12,889 DEBUG [pool-2-thread-3] org.apache.spark.sql.catalyst.catalog.SessionCatalog: Opening session with database: sales_analytics
2023-05-15 14:33:13,001 DEBUG [pool-2-thread-3] org.apache.hive.service.cli.operation.SQLOperation: Executing SQL: SELECT * FROM customers LIMIT 10
2023-05-15 14:33:13,112 DEBUG [pool-2-thread-3] org.apache.spark.sql.catalyst.parser.ParseDriver: Parsing SQL query
2023-05-15 14:33:13,223 DEBUG [pool-2-thread-3] org.apache.spark.sql.catalyst.analysis.Analyzer: Analyzing query
2023-05-15 14:33:13,334 DEBUG [pool-2-thread-3] org.apache.spark.sql.catalyst.catalog.HiveTableRelation: Reading table: sales_analytics.customers
2023-05-15 14:33:13,445 DEBUG [pool-2-thread-3] org.apache.spark.sql.execution.datasources.FileSourceStrategy: Planning file scan
2023-05-15 14:33:13,556 DEBUG [pool-2-thread-3] org.apache.spark.sql.execution.SparkPlan: Physical plan generated
2023-05-15 14:33:13,667 DEBUG [Executor task launch worker-0] org.apache.spark.executor.Executor: Running task 0.0 in stage 0.0
2023-05-15 14:33:13,778 DEBUG [Executor task launch worker-0] org.apache.spark.storage.BlockManager: Getting block rdd_1_0
2023-05-15 14:33:13,889 WARN  [pool-2-thread-3] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: Failed to compile expression, falling back to interpreted mode
java.lang.Exception: Compilation failed
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:156)
    at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:89)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
2023-05-15 14:33:14,001 DEBUG [pool-2-thread-3] org.apache.spark.sql.execution.SparkPlan: Falling back to interpreted execution
2023-05-15 14:33:14,112 DEBUG [Executor task launch worker-0] org.apache.spark.storage.MemoryStore: Block rdd_1_0 stored in memory (estimated size 4.2 MB)
2023-05-15 14:33:14,223 DEBUG [pool-2-thread-3] org.apache.hive.service.cli.operation.SQLOperation: Query completed successfully, rows: 10
2023-05-15 14:33:14,334 DEBUG [pool-2-thread-3] org.apache.hive.service.cli.session.HiveSessionImpl: Returning results to client
2023-05-15 14:33:45,445 DEBUG [Thread-7] org.apache.thrift.server.TThreadPoolServer: New connection from client 192.168.1.108
2023-05-15 14:33:45,556 DEBUG [Thread-7] org.apache.thrift.transport.TSocket: Client connected from 192.168.1.108:51267
2023-05-15 14:33:45,667 DEBUG [pool-2-thread-5] org.apache.hive.service.cli.session.SessionManager: Creating new session for user: analyst2
2023-05-15 14:33:45,778 DEBUG [pool-2-thread-5] org.apache.spark.sql.catalyst.catalog.SessionCatalog: Session database: sales_analytics
2023-05-15 14:33:45,889 DEBUG [pool-2-thread-5] org.apache.hive.service.cli.operation.SQLOperation: Executing SQL: USE test_db
2023-05-15 14:33:46,001 DEBUG [pool-2-thread-5] org.apache.spark.sql.catalyst.catalog.SessionCatalog: Switching to database: test_db
2023-05-15 14:33:46,112 DEBUG [pool-2-thread-5] org.apache.hive.service.cli.operation.SQLOperation: Database switched to test_db
2023-05-15 14:34:15,223 DEBUG [Thread-9] org.apache.thrift.transport.TSocket: Client connected from 192.168.1.112:51289
2023-05-15 14:34:15,334 DEBUG [pool-2-thread-7] org.apache.hive.service.cli.session.SessionManager: Creating new session for user: admin
2023-05-15 14:34:15,445 DEBUG [pool-2-thread-7] org.apache.hive.service.cli.operation.SQLOperation: Executing SQL: SHOW DATABASES
2023-05-15 14:34:15,556 DEBUG [pool-2-thread-7] org.apache.spark.sql.catalyst.catalog.HiveExternalCatalog: Listing databases
2023-05-15 14:34:15,667 DEBUG [pool-2-thread-7] org.apache.hadoop.hive.metastore.HiveMetaStoreClient: getAllDatabases()
2023-05-15 14:34:15,778 DEBUG [pool-2-thread-7] org.apache.hive.service.cli.operation.SQLOperation: Returned databases: default, information_schema, sales_analytics, temp_db, test_db
2023-05-15 14:34:30,889 DEBUG [pool-2-thread-9] org.apache.hive.service.cli.operation.SQLOperation: Executing SQL: SELECT COUNT(*) FROM sales_analytics.transactions WHERE date >= '2023-01-01'
2023-05-15 14:34:31,001 DEBUG [pool-2-thread-9] org.apache.spark.sql.catalyst.optimizer.Optimizer: Optimizing query plan
2023-05-15 14:34:31,112 DEBUG [pool-2-thread-9] org.apache.spark.sql.execution.aggregate.HashAggregateExec: Using hash-based aggregation
2023-05-15 14:34:31,223 DEBUG [Executor task launch worker-1] org.apache.spark.executor.Executor: Running task 0.0 in stage 1.0
2023-05-15 14:34:31,334 ERROR [Executor task launch worker-1] org.apache.spark.storage.BlockManager: Failed to get block rdd_2_5
java.io.IOException: Connection timeout
    at org.apache.spark.network.client.TransportClient.sendRpc(TransportClient.java:89)
    at org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:567)
2023-05-15 14:34:31,445 DEBUG [Executor task launch worker-1] org.apache.spark.storage.BlockManager: Retrying block fetch
2023-05-15 14:34:31,556 DEBUG [Executor task launch worker-1] org.apache.spark.storage.BlockManager: Block rdd_2_5 fetched successfully
2023-05-15 14:34:32,667 DEBUG [pool-2-thread-9] org.apache.spark.scheduler.DAGScheduler: Stage 1 completed successfully
2023-05-15 14:34:32,778 DEBUG [pool-2-thread-9] org.apache.hive.service.cli.operation.SQLOperation: Query completed, result: 1523478
2023-05-15 14:35:01,889 DEBUG [pool-3-thread-1] org.apache.spark.sql.execution.ui.SQLAppStatusListener: SQL execution metrics updated
2023-05-15 14:35:01,990 DEBUG [pool-3-thread-1] org.apache.spark.storage.MemoryStore: Memory usage: 234.5 MB / 2048.0 MB
2023-05-15 14:35:02,101 DEBUG [pool-3-thread-1] org.apache.spark.storage.BlockManager: Number of cached blocks: 45
2023-05-15 14:35:02,212 DEBUG [Thread-12] org.apache.thrift.transport.TSocket: Client connected from 192.168.1.115:51345
2023-05-15 14:35:02,323 DEBUG [pool-2-thread-11] org.apache.hive.service.cli.session.SessionManager: Active sessions: 5
2023-05-15 14:35:02,434 DEBUG [pool-2-thread-11] org.apache.hive.service.cli.session.SessionManager: Idle sessions: 2
2023-05-15 14:35:30,545 DEBUG [pool-2-thread-13] org.apache.hive.service.cli.operation.SQLOperation: Executing SQL: CREATE TABLE IF NOT EXISTS temp_db.staging_data
2023-05-15 14:35:30,656 DEBUG [pool-2-thread-13] org.apache.spark.sql.catalyst.catalog.HiveExternalCatalog: Creating table in database: temp_db
2023-05-15 14:35:30,767 WARN  [pool-2-thread-13] org.apache.hadoop.hive.metastore.HiveMetaStoreClient: Table already exists in temp_db
2023-05-15 14:35:30,878 DEBUG [pool-2-thread-13] org.apache.hive.service.cli.operation.SQLOperation: Table creation skipped (already exists)
2023-05-15 14:36:15,989 DEBUG [Thread-15] org.apache.thrift.transport.TSocket: Client disconnected: 192.168.1.105:51234
2023-05-15 14:36:16,100 DEBUG [pool-2-thread-3] org.apache.hive.service.cli.session.HiveSessionImpl: Closing session: SessionHandle [12345-abcd-6789]
2023-05-15 14:36:16,211 DEBUG [pool-2-thread-3] org.apache.spark.sql.catalyst.catalog.SessionCatalog: Closing session catalog
2023-05-15 14:36:45,322 DEBUG [heartbeat-thread] org.apache.spark.executor.Executor: Executor heartbeat sent
2023-05-15 14:36:45,433 DEBUG [heartbeat-thread] org.apache.spark.storage.BlockManagerMaster: Block manager heartbeat sent
2023-05-15 14:37:01,544 DEBUG [pool-4-thread-1] org.apache.spark.sql.execution.ui.SQLAppStatusListener: Active SQL queries: 3
2023-05-15 14:37:01,655 DEBUG [pool-4-thread-1] org.apache.spark.sql.execution.ui.SQLAppStatusListener: Completed SQL queries: 127
2023-05-15 14:37:30,766 DEBUG [Thread-18] org.apache.thrift.transport.TSocket: Client connected from 10.0.0.45:52101
2023-05-15 14:37:30,877 DEBUG [pool-2-thread-17] org.apache.hive.service.cli.session.SessionManager: Creating new session for user: etl_service
2023-05-15 14:37:30,988 DEBUG [pool-2-thread-17] org.apache.spark.sql.catalyst.catalog.SessionCatalog: Opening session with default database: sales_analytics
2023-05-15 14:37:31,099 DEBUG [pool-2-thread-17] org.apache.hive.service.cli.operation.SQLOperation: Executing SQL: INSERT INTO sales_analytics.daily_summary SELECT date, SUM(amount) FROM sales_analytics.transactions GROUP BY date
2023-05-15 14:37:31,210 DEBUG [pool-2-thread-17] org.apache.spark.sql.catalyst.optimizer.Optimizer: Applying optimization rules
2023-05-15 14:37:31,321 DEBUG [pool-2-thread-17] org.apache.spark.sql.execution.exchange.ShuffleExchangeExec: Performing shuffle exchange
2023-05-15 14:38:15,432 DEBUG [Executor task launch worker-5] org.apache.spark.executor.Executor: Task 0.0 in stage 5.0 finished successfully
2023-05-15 14:38:15,543 DEBUG [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler: Stage 5 completed with 200 tasks
2023-05-15 14:38:15,654 DEBUG [pool-2-thread-17] org.apache.hive.service.cli.operation.SQLOperation: Insert operation completed, rows affected: 365
2023-05-15 14:38:45,765 DEBUG [pool-5-thread-1] org.apache.spark.storage.BlockManager: Evicting block rdd_3_12 from memory
2023-05-15 14:38:45,876 DEBUG [pool-5-thread-1] org.apache.spark.storage.MemoryStore: Block rdd_3_12 evicted, size: 8.3 MB
2023-05-15 14:39:00,987 DEBUG [Thread-22] org.apache.thrift.transport.TSocket: Connection timeout on socket, closing
2023-05-15 14:39:01,098 DEBUG [pool-2-thread-19] org.apache.hive.service.cli.session.HiveSessionImpl: Session timeout detected, cleaning up resources
2023-05-15 14:39:30,209 DEBUG [main-watcher] org.apache.hadoop.conf.Configuration: spark.sql.warehouse.dir = /opt/spark/warehouse
2023-05-15 14:39:30,320 DEBUG [main-watcher] org.apache.hadoop.conf.Configuration: spark.driver.memory = 4g
2023-05-15 14:39:30,431 DEBUG [main-watcher] org.apache.hadoop.conf.Configuration: spark.executor.memory = 8g
2023-05-15 14:39:30,542 DEBUG [main-watcher] org.apache.hadoop.conf.Configuration: spark.sql.catalogImplementation = hive