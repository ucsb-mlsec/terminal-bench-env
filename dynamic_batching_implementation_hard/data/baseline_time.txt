Baseline Performance Results
==================================

Processing method: Sequential (one tensor at a time)

Total tensors processed: 100
Total time: 12.47 seconds
Average per tensor: 0.1247 seconds

Performance notes:
- Each tensor is loaded, processed, and saved individually
- Model is invoked 100 separate times (one per tensor)
- No batching or parallelization utilized
- Significant overhead from repeated model invocations
- GPU/CPU underutilized due to small individual workloads

Bottleneck analysis:
- Model initialization overhead per call: ~0.02s
- Data loading overhead per tensor: ~0.01s
- Actual inference time per tensor: ~0.09s
- Result serialization per tensor: ~0.005s

Recommendation: Batch processing could significantly reduce overhead
by amortizing model invocation costs across multiple tensors.