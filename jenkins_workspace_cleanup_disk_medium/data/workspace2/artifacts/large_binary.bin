#!/usr/bin/env python3
import os
import json
from datetime import datetime, timedelta
from pathlib import Path

# Create the workspace directory structure
workspace_base = Path("/var/lib/jenkins/workspace")
workspace_base.mkdir(parents=True, exist_ok=True)

# Create the artifacts directory for data/workspace2
artifacts_dir = workspace_base / "data" / "workspace2" / "artifacts"
artifacts_dir.mkdir(parents=True, exist_ok=True)

# Create the large_binary.bin file
binary_file = artifacts_dir / "large_binary.bin"

# Generate 600MB of content (using sparse file technique for simulation)
# We'll create actual content to make it realistic
file_size_mb = 600
chunk_size = 1024 * 1024  # 1MB chunks
header = b"JENKINS_ARTIFACT_BINARY_FILE_v1.0\n"
header += b"Generated: 2024-01-01 00:00:00\n"
header += b"Size: 600MB\n"
header += b"Type: Compiled Binary Artifact\n"
header += b"Job: data-processing-pipeline\n"
header += b"Build: #437\n"
header += b"\x00" * 100  # Null bytes separator

with open(binary_file, "wb") as f:
    # Write header
    f.write(header)
    
    # Write repeating pattern to simulate binary content
    pattern = bytes(range(256)) * 4096  # 1MB pattern
    
    for i in range(file_size_mb):
        f.write(pattern)

# Set the modification time to 45 days ago
days_ago = 45
old_time = datetime.now() - timedelta(days=days_ago)
timestamp = old_time.timestamp()
os.utime(binary_file, (timestamp, timestamp))

# Create additional workspace directories with various sizes and ages
workspaces = [
    ("data/workspace1", 350, 20),
    ("data/workspace2", 620, 45),  # Already created above
    ("data/workspace3", 150, 10),
    ("frontend/build-app", 550, 35),
    ("frontend/test-suite", 80, 5),
    ("backend/api-service", 720, 40),
    ("backend/database-migration", 200, 50),
    ("ml-pipeline/training-job", 890, 60),
    ("ml-pipeline/inference-service", 100, 15),
    ("legacy/old-project-1", 650, 90),
    ("legacy/old-project-2", 480, 100),
    ("active/current-sprint", 300, 2),
]

for workspace_name, size_mb, age_days in workspaces:
    if workspace_name == "data/workspace2":
        continue  # Already created
    
    workspace_path = workspace_base / workspace_name
    workspace_path.mkdir(parents=True, exist_ok=True)
    
    # Create a file to represent the workspace size
    data_file = workspace_path / "build_data.bin"
    
    # Write data to approximate the size
    with open(data_file, "wb") as f:
        pattern = b"BUILD_ARTIFACT_DATA" * 1000
        chunks_needed = (size_mb * 1024 * 1024) // len(pattern)
        for _ in range(chunks_needed):
            f.write(pattern)
    
    # Set modification time
    old_time = datetime.now() - timedelta(days=age_days)
    timestamp = old_time.timestamp()
    os.utime(data_file, (timestamp, timestamp))
    os.utime(workspace_path, (timestamp, timestamp))

# Now analyze the workspace and generate the report
def get_directory_size(path):
    """Calculate total size of directory in bytes"""
    total = 0
    try:
        for entry in os.scandir(path):
            if entry.is_file(follow_symlinks=False):
                total += entry.stat().st_size
            elif entry.is_dir(follow_symlinks=False):
                total += get_directory_size(entry.path)
    except PermissionError:
        pass
    return total

def get_last_modified(path):
    """Get the most recent modification time in a directory"""
    try:
        latest = os.path.getmtime(path)
        for entry in os.scandir(path):
            if entry.is_file(follow_symlinks=False):
                mtime = entry.stat().st_mtime
                if mtime > latest:
                    latest = mtime
            elif entry.is_dir(follow_symlinks=False):
                dir_latest = get_last_modified(entry.path)
                if dir_latest > latest:
                    latest = dir_latest
        return latest
    except:
        return 0

# Analyze workspaces
total_size_bytes = 0
large_workspaces = []
old_workspaces = []
reclaimable = []

cutoff_size_bytes = 500 * 1024 * 1024  # 500MB in bytes
cutoff_time = datetime.now() - timedelta(days=30)
cutoff_timestamp = cutoff_time.timestamp()

# Get all immediate subdirectories (first level workspaces)
workspace_dirs = []
for category in os.listdir(workspace_base):
    category_path = workspace_base / category
    if category_path.is_dir():
        for workspace in os.listdir(category_path):
            workspace_path = category_path / workspace
            if workspace_path.is_dir():
                workspace_dirs.append(workspace_path)

for workspace_path in workspace_dirs:
    size_bytes = get_directory_size(workspace_path)
    total_size_bytes += size_bytes
    
    last_modified = get_last_modified(workspace_path)
    
    is_large = size_bytes > cutoff_size_bytes
    is_old = last_modified < cutoff_timestamp
    
    if is_large:
        large_workspaces.append((workspace_path, size_bytes))
    
    if is_old:
        old_workspaces.append((workspace_path, last_modified))
    
    if is_large and is_old:
        reclaimable.append((workspace_path, size_bytes))

# Calculate totals
total_size_mb = int(total_size_bytes / (1024 * 1024))
large_count = len(large_workspaces)
old_count = len(old_workspaces)
reclaimable_bytes = sum(size for _, size in reclaimable)
reclaimable_mb = int(reclaimable_bytes / (1024 * 1024))

# Generate report
report = {
    "total_workspace_size_mb": total_size_mb,
    "large_workspaces_count": large_count,
    "old_workspaces_count": old_count,
    "reclaimable_space_mb": reclaimable_mb
}

# Write report to file
output_file = Path("/tmp/jenkins_cleanup_report.json")
with open(output_file, "w") as f:
    json.dump(report, f, indent=2)

print(f"Jenkins workspace cleanup report generated at {output_file}")
print(f"Total workspace size: {total_size_mb} MB")
print(f"Large workspaces (>500MB): {large_count}")
print(f"Old workspaces (30+ days): {old_count}")
print(f"Reclaimable space: {reclaimable_mb} MB")