import numpy as np

# Set random seed for reproducibility
np.random.seed(42)

# Generate 128 samples with 128-dimensional embeddings
n_samples = 128
embedding_dim = 128

# Create base embeddings from random normal distribution
embeddings = np.random.randn(n_samples, embedding_dim).astype(np.float32)

# Normalize all embeddings to unit length (L2 norm = 1)
norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
embeddings = embeddings / norms

# Create some clusters with very high similarity to trigger numerical issues
# This simulates what happens when the model becomes confident

# Cluster 1: samples 0-15 should be very similar
cluster1_base = np.random.randn(embedding_dim).astype(np.float32)
cluster1_base = cluster1_base / np.linalg.norm(cluster1_base)
for i in range(16):
    noise = np.random.randn(embedding_dim).astype(np.float32) * 0.15
    embeddings[i] = cluster1_base + noise
    embeddings[i] = embeddings[i] / np.linalg.norm(embeddings[i])

# Cluster 2: samples 16-31 should be very similar
cluster2_base = np.random.randn(embedding_dim).astype(np.float32)
cluster2_base = cluster2_base / np.linalg.norm(cluster2_base)
for i in range(16, 32):
    noise = np.random.randn(embedding_dim).astype(np.float32) * 0.15
    embeddings[i] = cluster2_base + noise
    embeddings[i] = embeddings[i] / np.linalg.norm(embeddings[i])

# Cluster 3: samples 32-47 should be very similar
cluster3_base = np.random.randn(embedding_dim).astype(np.float32)
cluster3_base = cluster3_base / np.linalg.norm(cluster3_base)
for i in range(32, 48):
    noise = np.random.randn(embedding_dim).astype(np.float32) * 0.12
    embeddings[i] = cluster3_base + noise
    embeddings[i] = embeddings[i] / np.linalg.norm(embeddings[i])

# Add some very confident pairs (cosine similarity > 0.9)
# This will cause exp(sim/0.07) to overflow
for i in range(48, 58):
    base = np.random.randn(embedding_dim).astype(np.float32)
    base = base / np.linalg.norm(base)
    noise = np.random.randn(embedding_dim).astype(np.float32) * 0.08
    embeddings[i] = base + noise
    embeddings[i] = embeddings[i] / np.linalg.norm(embeddings[i])

# Add a few almost identical pairs that will definitely cause overflow
for i in range(58, 64):
    base = np.random.randn(embedding_dim).astype(np.float32)
    base = base / np.linalg.norm(base)
    noise = np.random.randn(embedding_dim).astype(np.float32) * 0.05
    embeddings[i] = base + noise
    embeddings[i] = embeddings[i] / np.linalg.norm(embeddings[i])

# Keep the rest more spread out but still normalized
for i in range(64, n_samples):
    embeddings[i] = embeddings[i] / np.linalg.norm(embeddings[i])

# Final normalization pass to ensure all embeddings have unit norm
for i in range(n_samples):
    embeddings[i] = embeddings[i] / np.linalg.norm(embeddings[i])

# Verify properties
print(f"Shape: {embeddings.shape}")
print(f"Dtype: {embeddings.dtype}")
print(f"Sample norms: {np.linalg.norm(embeddings[:5], axis=1)}")

# Check similarity matrix to ensure we have high similarities
similarities = np.matmul(embeddings, embeddings.T)
print(f"Max similarity: {np.max(similarities[~np.eye(n_samples, dtype=bool)])}")
print(f"Min similarity: {np.min(similarities)}")
print(f"Mean similarity: {np.mean(similarities[~np.eye(n_samples, dtype=bool)])}")

# Save the embeddings
np.save('/tmp/embeddings.npy', embeddings)