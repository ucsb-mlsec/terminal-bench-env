# SLURM Configuration File
# Cluster: hpc-cluster
# Generated for HPC environment

# CLUSTER IDENTIFICATION
ClusterName=hpc-cluster
ControlMachine=slurmctld-node

# SLURM DAEMONS
SlurmctldPort=6817
SlurmdPort=6818
SlurmUser=slurm
SlurmdUser=root

# FILE PATHS
StateSaveLocation=/var/spool/slurmctld
SlurmdSpoolDir=/var/spool/slurmd
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid

# LOGGING
SlurmctldDebug=info
SlurmdDebug=info

# TIMEOUT AND COMMUNICATION PARAMETERS
MessageTimeout=5
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0

# RESOURCE SELECTION
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory

# TASK LAUNCH
TaskPlugin=task/affinity,task/cgroup

# JOB SCHEDULING
SchedulerType=sched/backfill
PriorityType=priority/multifactor
PriorityDecayHalfLife=7-0
PriorityMaxAge=7-0
PriorityWeightAge=1000
PriorityWeightFairshare=10000
PriorityWeightJobSize=1000
PriorityWeightPartition=1000
PriorityWeightQOS=0

# ACCOUNTING
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=localhost
AccountingStoragePort=6819
AccountingStorageEnforce=associations,limits,qos
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30

# PROCESS TRACKING
ProctrackType=proctrack/cgroup

# COMPLETION AND REQUEUE
CompleteWait=0
ReturnToService=1

# RESOURCE LIMITS
PropagateResourceLimitsExcept=MEMLOCK
MaxJobCount=10000
MaxArraySize=1001

# HEALTH CHECK
HealthCheckProgram=/usr/sbin/nhc
HealthCheckInterval=300

# NODE CONFIGURATION
# 80 compute nodes with uniform configuration
NodeName=compute[001-080] CPUs=32 RealMemory=128000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=1 State=UNKNOWN

# PARTITION CONFIGURATION
PartitionName=general Nodes=compute[001-080] Default=YES MaxTime=48:00:00 State=UP Priority=50 OverSubscribe=NO