{"query": "how to rollback kubernetes deployment", "candidates": [{"id": "doc_1", "text": "Kubernetes Deployment Best Practices: When managing applications in Kubernetes, deployments are the primary way to manage your application lifecycle. A deployment provides declarative updates for pods and replica sets. You can describe a desired state in a deployment, and the deployment controller changes the actual state to the desired state at a controlled rate. Deployments allow you to scale applications, perform rolling updates, and yes, even rollback to previous versions if needed. The rollback feature is mentioned in passing as one of many deployment capabilities available to cluster administrators."}, {"id": "doc_2", "text": "Rolling Back Kubernetes Deployments: A Complete Guide. When a deployment goes wrong, you need to rollback quickly. Use 'kubectl rollout undo deployment/my-deployment' to rollback to the previous revision. You can also rollback to a specific revision using 'kubectl rollout undo deployment/my-deployment --to-revision=2'. To check rollout history, use 'kubectl rollout history deployment/my-deployment'. Before rolling back, verify the current status with 'kubectl rollout status deployment/my-deployment'. The rollback process will gradually replace pods with the previous version, ensuring zero downtime. You can pause a rollout with 'kubectl rollout pause' and resume with 'kubectl rollout resume'. Always check your revision history and validate the target revision before executing a rollback."}, {"id": "doc_3", "text": "Container orchestration has revolutionized how we deploy applications. Kubernetes, Docker Swarm, and other platforms provide robust ways to manage containerized workloads. Understanding the basics of how these systems work is crucial for modern DevOps practices."}, {"id": "doc_4", "text": "Kubernetes Architecture Overview: Kubernetes consists of a control plane and worker nodes. The control plane includes the API server, scheduler, and controller manager. Worker nodes run your application pods. Understanding this architecture is fundamental to working with Kubernetes effectively."}, {"id": "doc_5", "text": "Deployment strategies in cloud environments vary widely. Blue-green deployments, canary releases, and rolling updates each have their place. Choosing the right strategy depends on your application's requirements, your tolerance for downtime, and your rollback needs."}], "relevant_id": "doc_2"}
{"query": "debugging null pointer exceptions in java", "candidates": [{"id": "doc_1", "text": "Java Exceptions Overview: Java provides a comprehensive exception handling mechanism. Exceptions are divided into checked and unchecked exceptions. The exception hierarchy starts with Throwable, which has two main subclasses: Error and Exception. Common exceptions include IOException, SQLException, and RuntimeException. NullPointerException is one of the most common runtime exceptions developers encounter, though it's just one of many exception types you'll need to handle in production code."}, {"id": "doc_2", "text": "Debugging NullPointerException: A Practical Guide. NullPointerException (NPE) occurs when you try to use a reference that points to no location in memory (null) as though it were referencing an object. To debug NPEs: First, identify the exact line using the stack trace. Second, check which variable is null by examining all object references on that line. Use your IDE's debugger to inspect variable values at runtime. Add null checks before dereferencing objects. Consider using Optional<T> in Java 8+ to handle potentially null values more gracefully. Enable assertions during development with -ea flag. Use static analysis tools like SpotBugs or NullAway to catch potential NPEs before runtime. Common causes include uninitialized instance variables, method return values, and collection elements. Always validate input parameters and use Objects.requireNonNull() for critical non-null requirements."}, {"id": "doc_3", "text": "Object-oriented programming in Java relies heavily on references and objects. Understanding how memory allocation works, how the garbage collector manages objects, and how references are passed between methods is essential for writing robust Java applications."}, {"id": "doc_4", "text": "Java Development Environment Setup: Installing JDK, configuring IDE, setting up Maven or Gradle, and establishing coding standards are the first steps in any Java project. A well-configured development environment improves productivity and reduces common errors."}, {"id": "doc_5", "text": "Exception handling best practices include catching specific exceptions rather than generic ones, logging exceptions appropriately, and never swallowing exceptions silently. Always include meaningful error messages and context information."}, {"id": "doc_6", "text": "Java memory management and the lifecycle of objects is controlled by the JVM. Understanding heap vs stack, garbage collection algorithms, and memory leaks helps you write more efficient code."}], "relevant_id": "doc_2"}
{"query": "fix broken docker build", "candidates": [{"id": "doc_1", "text": "Docker architecture overview and container basics. Docker uses a client-server architecture where the Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing containers. Understanding how images, containers, and registries work together is fundamental to using Docker effectively in production environments."}, {"id": "doc_2", "text": "Troubleshooting Docker Build Failures: Common Errors and Solutions. When docker build fails, start by reading the error message carefully - it usually points to the failing layer. Common issues include: 1) Network problems during package installation - add retry logic or use --network=host. 2) Missing dependencies - ensure your base image includes required tools. 3) Permission errors - check file ownership and use appropriate USER directives. 4) Cache issues - try --no-cache flag to rebuild from scratch. 5) Dockerfile syntax errors - validate your Dockerfile with linters. 6) Context size too large - use .dockerignore to exclude unnecessary files. Check disk space with 'docker system df'. Clear old images with 'docker system prune'. Review build logs with 'docker build --progress=plain'. Test individual RUN commands in a temporary container. Verify base image availability and version compatibility."}, {"id": "doc_3", "text": "Microservices architecture patterns involve breaking down applications into smaller, independent services. Each service runs in its own process and communicates through well-defined APIs, often deployed as containers."}, {"id": "doc_4", "text": "Docker Compose allows you to define multi-container applications using YAML files. It simplifies the process of managing multiple related containers and their networking configurations."}, {"id": "doc_5", "text": "Container security best practices include scanning images for vulnerabilities, using minimal base images, running containers as non-root users, and keeping images updated with security patches."}], "relevant_id": "doc_2"}
{"query": "kubernetes pod crashing with CrashLoopBackOff", "candidates": [{"id": "doc_1", "text": "Debugging CrashLoopBackOff and Pod Failures in Kubernetes. CrashLoopBackOff means your pod is starting, crashing, and Kubernetes is repeatedly trying to restart it with exponential backoff. To debug: Use 'kubectl describe pod <pod-name>' to see events and recent status. Check logs with 'kubectl logs <pod-name>' or 'kubectl logs <pod-name> --previous' for logs from the crashed container. Common causes include: application errors on startup, missing environment variables or config maps, insufficient memory or CPU limits causing OOM kills, failed liveness/readiness probes, incorrect container commands or arguments, missing dependencies or files. Verify your container image runs locally with docker run. Check resource limits in your deployment spec. Review probe configurations - they might be too aggressive. Look for ImagePullBackOff which often precedes CrashLoopBackOff. Use 'kubectl get events' for cluster-wide event history. Exec into a similar pod to test configurations if possible."}, {"id": "doc_2", "text": "Kubernetes Scheduling and Resource Management: The Kubernetes scheduler assigns pods to nodes based on resource requirements and constraints. Understanding how scheduling works, including node selectors, affinity rules, and taints/tolerations, helps ensure your workloads run efficiently."}, {"id": "doc_3", "text": "Container health checks and monitoring are essential for production systems. Implementing proper observability through metrics, logs, and traces allows you to identify and resolve issues quickly."}, {"id": "doc_4", "text": "Introduction to container orchestration platforms. Modern applications require sophisticated management of containerized workloads. Platforms like Kubernetes provide automated deployment, scaling, and management capabilities that are essential for cloud-native applications."}, {"id": "doc_5", "text": "Kubernetes networking fundamentals cover pod-to-pod communication, service discovery, and ingress controllers. Each pod gets its own IP address, and services provide stable endpoints for accessing pods."}, {"id": "doc_6", "text": "The pod lifecycle in Kubernetes includes pending, running, succeeded, failed, and unknown states. Pods are the smallest deployable units and can contain one or more containers that share resources."}], "relevant_id": "doc_1"}
{"query": "configure postgres connection pooling", "candidates": [{"id": "doc_1", "text": "PostgreSQL Connection Pooling Configuration Guide. Connection pooling reduces overhead by reusing database connections. For application-level pooling in Python, use psycopg2 with a pool: 'from psycopg2 import pool; connection_pool = pool.SimpleConnectionPool(minconn=1, maxconn=20, host='localhost', database='mydb', user='user', password='pass')'. For Java, configure HikariCP: set maximumPoolSize (default 10), minimumIdle (default same as max), connectionTimeout (default 30s), and idleTimeout (default 10min). For standalone pooling, use PgBouncer: install it between application and PostgreSQL, configure pool_mode (session, transaction, or statement), set max_client_conn and default_pool_size in pgbouncer.ini. Monitor pool performance with PgBouncer SHOW POOLS command. Transaction mode offers best performance but requires stateless transactions. Tune pool size based on your workload - typically num_cores * 2 + effective_spindle_count. Consider connection lifetime and validation queries to handle stale connections."}, {"id": "doc_2", "text": "PostgreSQL Performance Tuning: Optimizing your PostgreSQL database involves multiple factors including proper indexing, query optimization, memory configuration, and connection management. Shared buffers, work_mem, and effective_cache_size are key parameters to tune."}, {"id": "doc_3", "text": "Database design principles for PostgreSQL include normalization, choosing appropriate data types, and designing efficient schemas. Good database design is the foundation of application performance."}, {"id": "doc_4", "text": "Setting up PostgreSQL replication for high availability. Streaming replication allows you to maintain one or more standby servers that stay synchronized with the primary server, providing failover capabilities."}, {"id": "doc_5", "text": "SQL query optimization techniques: using EXPLAIN ANALYZE, creating appropriate indexes, avoiding N+1 queries, and understanding query execution plans are essential skills for database developers."}], "relevant_id": "doc_1"}
{"query": "explain git rebase vs merge", "candidates": [{"id": "doc_1", "text": "Git Rebase vs Merge: Understanding the Difference. Both merge and rebase integrate changes from one branch into another, but they do it differently. Merge creates a new commit that combines both branches, preserving the complete history and branch structure - you'll see where branches diverged and came back together. Use 'git merge feature-branch' to merge. The history shows parallel development. Rebase moves your branch commits to the tip of the target branch, creating a linear history. Use 'git rebase main' to rebase your current branch onto main. This rewrites commit history by creating new commits with the same changes. Benefits of merge: preserves complete history, safe for public branches, shows collaboration clearly. Benefits of rebase: cleaner linear history, easier to follow, no merge commits cluttering history. Never rebase public/shared branches - only rebase local commits not yet pushed. For feature branches, rebase before merging to main for clean history. Use interactive rebase 'git rebase -i' to squash or reorder commits. Golden rule: rebase private branches, merge public branches."}, {"id": "doc_2", "text": "Git branching strategies for teams include Git Flow, GitHub Flow, and trunk-based development. Each strategy has its own conventions for creating branches, merging code, and releasing software."}, {"id": "doc_3", "text": "Version control systems have evolved from CVS and SVN to distributed systems like Git and Mercurial. Understanding the fundamentals of version control is essential for modern software development teams."}, {"id": "doc_4", "text": "Git basics: repositories, commits, and branches. Every Git repository contains the complete history of your project. Commits are snapshots of your project at specific points in time, and branches allow parallel development."}, {"id": "doc_5", "text": "Resolving merge conflicts in Git requires understanding both sets of changes and manually editing files to combine them properly. Git marks conflicts in files with special markers that you must resolve."}], "relevant_id": "doc_1"}
{"query": "python memory leak detection", "candidates": [{"id": "doc_1", "text": "Python Memory Profiling and Leak Detection. Memory leaks in Python usually occur due to circular references, unclosed resources, or global collections that grow unbounded. To detect leaks: Use memory_profiler with @profile decorator to track memory usage line-by-line: 'pip install memory_profiler' then 'python -m memory_profiler script.py'. Use tracemalloc (built-in Python 3.4+): 'import tracemalloc; tracemalloc.start()' then take snapshots and compare them. Pympler provides detailed memory analysis: 'from pympler import muppy, summary; all_objects = muppy.get_objects(); summary.print_(summary.summarize(all_objects))'. Objgraph visualizes object references: 'import objgraph; objgraph.show_most_common_types()'. Common leak patterns: global lists/dicts that grow indefinitely, circular references (though gc handles most), unclosed file handles or database connections, event listeners not removed, cache without size limits. Use weakref for caches. Implement __del__ carefully. Use context managers for resource cleanup. Profile with 'guppy3' for heap analysis. Monitor production with process memory metrics."}, {"id": "doc_2", "text": "Python performance optimization involves multiple strategies including algorithm choice, data structure selection, and proper use of built-in functions. Understanding time and space complexity helps you write efficient code."}, {"id": "doc_3", "text": "Memory management in Python is handled automatically by the garbage collector. Python uses reference counting and a cyclic garbage collector to reclaim memory from objects that are no longer accessible."}, {"id": "doc_4", "text": "Debugging Python applications with pdb, the Python debugger. Set breakpoints, step through code, inspect variables, and understand program flow using pdb's interactive debugging capabilities."}, {"id": "doc_5", "text": "Python best practices for writing maintainable code include following PEP 8 style guide, writing comprehensive tests, using type hints, and documenting your code properly."}, {"id": "doc_6", "text": "Virtual environments in Python isolate project dependencies and prevent conflicts between different projects. Use venv or virtualenv to create isolated Python environments for each project."}], "relevant_id": "doc_1"}
{"query": "jenkins pipeline timeout issue", "candidates": [{"id": "doc_1", "text": "Jenkins Pipeline Timeout Configuration and Troubleshooting. Pipeline timeouts can occur at multiple levels. To configure timeouts: Use 'timeout' step in Jenkinsfile: 'timeout(time: 30, unit: 'MINUTES') { //pipeline code }'. Set global timeout in Jenkins configuration under 'Manage Jenkins > Configure System'. For agent-level timeouts, configure in agent settings. Common timeout issues: long-running tests - split into parallel stages, slow dependency downloads - use caching or mirrors, network latency - increase timeout values, insufficient resources - check agent capacity. Debug with timestamps: 'options { timestamps() }'. Identify slow stages with 'Pipeline: Stage View' plugin. Use 'timeout' at stage level for granular control. Check Jenkins logs for timeout messages. Consider: Is timeout too aggressive? Is the job actually stuck? Are resources constrained? Implement retry logic for flaky steps: 'retry(3) { //code }'. Use parallel stages to reduce total time. Cache dependencies with tools like Artifactory. Monitor agent health and connectivity."}, {"id": "doc_2", "text": "Jenkins Pipeline Syntax and Best Practices: Declarative vs Scripted pipelines each have their advantages. Declarative pipelines provide a simpler, more structured syntax while scripted pipelines offer more flexibility through Groovy code."}, {"id": "doc_3", "text": "CI/CD fundamentals and continuous integration principles. Automated testing, continuous delivery, and deployment pipelines have transformed how software teams ship code to production."}, {"id": "doc_4", "text": "Distributed build systems allow you to scale Jenkins by adding multiple agents. Configure agents with different labels to run specific types of jobs on appropriate infrastructure."}, {"id": "doc_5", "text": "Jenkins plugin ecosystem provides extensive functionality. Popular plugins include Pipeline, Blue Ocean, Git, Docker, and Kubernetes plugins that extend Jenkins capabilities."}], "relevant_id": "doc_1"}
{"query": "aws s3 bucket policy for public read access", "candidates": [{"id": "doc_1", "text": "AWS S3 Bucket Policy for Public Read Access. To make S3 objects publicly readable, apply this bucket policy: '{ \"Version\": \"2012-10-17\", \"Statement\": [{ \"Sid\": \"PublicReadGetObject\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::your-bucket-name/*\" }] }'. Before applying: Disable 'Block all public access' settings in bucket permissions. Consider security implications - only make buckets public if necessary. For static website hosting, also enable static website hosting in bucket properties and set index document. Alternative approaches: Use CloudFront with S3 origin for better performance and security. Use presigned URLs for temporary public access. Use AWS IAM roles for programmatic access instead. Verify policy with AWS Policy Simulator. Monitor access with S3 access logging and CloudTrail. Set object ACLs to 'public-read' if per-object control needed. Remember: anyone can download objects from public buckets, so never store sensitive data in public buckets."}, {"id": "doc_2", "text": "AWS S3 Storage Classes and Lifecycle Policies: S3 offers multiple storage classes including Standard, Intelligent-Tiering, Glacier, and Deep Archive. Choose based on access patterns and cost requirements."}, {"id": "doc_3", "text": "Cloud storage fundamentals across AWS, Azure, and Google Cloud. Object storage has become the standard for storing unstructured data in cloud environments, offering scalability and durability."}, {"id": "doc_4", "text": "S3 versioning and backup strategies help protect data from accidental deletion or overwrites. Enable versioning on critical buckets and configure lifecycle policies for old versions."}, {"id": "doc_5", "text": "AWS Identity and Access Management (IAM) controls access to AWS resources. Understanding policies, roles, and permissions is crucial for securing your AWS infrastructure."}, {"id": "doc_6", "text": "Data encryption in S3 includes server-side encryption (SSE-S3, SSE-KMS, SSE-C) and client-side encryption. Choose encryption method based on your compliance and security requirements."}], "relevant_id": "doc_1"}
{"query": "react hooks useEffect infinite loop", "candidates": [{"id": "doc_1", "text": "Fixing useEffect Infinite Loops in React. Infinite loops with useEffect occur when the effect updates a state variable that's listed in the dependency array, causing the effect to run again. Common causes and fixes: 1) Missing dependency array - always include second argument: 'useEffect(() => {}, [dependencies])'. Empty array '[]' runs once on mount. 2) Object/array dependencies created inline - move outside component or use useCallback/useMemo: 'const obj = useMemo(() => ({key: value}), [value])'. 3) Updating state that's in dependencies - split into multiple effects or use functional state updates: 'setState(prev => prev + 1)'. 4) Async functions in useEffect - define function inside effect or use useCallback. 5) Incorrect dependency array - include all values from component scope used in effect. Use ESLint plugin 'eslint-plugin-react-hooks' to catch dependency issues. Use useRef for values that shouldn't trigger re-renders. Consider useReducer for complex state logic. Debug by logging when effect runs. Check React DevTools Profiler for render frequency."}, {"id": "doc_2", "text": "React Hooks Overview: useState, useEffect, useContext, useReducer, useCallback, useMemo, and custom hooks provide powerful ways to add state and side effects to functional components without writing classes."}, {"id": "doc_3", "text": "Component lifecycle in React includes mounting, updating, and unmounting phases. Understanding lifecycle helps you know when to perform operations like data fetching or subscriptions."}, {"id": "doc_4", "text": "React performance optimization techniques include memoization with React.memo, code splitting with React.lazy, virtualization for long lists, and proper key usage in lists."}, {"id": "doc_5", "text": "State management in React applications can be handled with local state, Context API, Redux, MobX, or Zustand. Choose based on application complexity and team preferences."}], "relevant_id": "doc_1"}
{"query": "sql query optimization slow performance", "candidates": [{"id": "doc_1", "text": "SQL Query Optimization for Slow Queries. Start by analyzing query performance: Use EXPLAIN or EXPLAIN ANALYZE to see execution plan. Look for sequential scans on large tables - add indexes. Check for expensive operations like sorts, nested loops, or hash joins. Optimization techniques: 1) Index foreign keys and WHERE clause columns. 2) Avoid SELECT * - specify needed columns. 3) Use EXISTS instead of IN for subqueries. 4) Limit result sets with WHERE before JOIN. 5) Denormalize when appropriate for read-heavy workloads. 6) Use covering indexes to avoid table lookups. 7) Partition large tables by date or key ranges. 8) Update statistics regularly for accurate query plans. 9) Avoid functions on indexed columns in WHERE clauses. 10) Use UNION ALL instead of UNION when duplicates acceptable. Monitor query performance with database-specific tools: pg_stat_statements for PostgreSQL, Query Store for SQL Server, Performance Schema for MySQL. Consider caching frequent queries at application level. Profile before optimizing - measure actual bottlenecks."}, {"id": "doc_2", "text": "Database indexing fundamentals: B-tree indexes, hash indexes, and specialized index types each serve different purposes. Understanding when to use which index type is crucial for performance."}, {"id": "doc_3", "text": "Relational database design and normalization reduces data redundancy and improves data integrity. Normal forms (1NF, 2NF, 3NF) guide the design process for efficient database schemas."}, {"id": "doc_4", "text": "Transaction isolation levels in SQL databases include Read Uncommitted, Read Committed, Repeatable Read, and Serializable. Each level provides different guarantees about concurrent access."}, {"id": "doc_5", "text": "Database backup and recovery strategies ensure data safety. Regular backups, point-in-time recovery, and disaster recovery planning are essential for production databases."}, {"id": "doc_6", "text": "NoSQL databases like MongoDB, Cassandra, and Redis offer alternatives to traditional relational databases for specific use cases requiring scalability or flexible schemas."}], "relevant_id": "doc_1"}
{"query": "configure nginx reverse proxy with ssl", "candidates": [{"id": "doc_1", "text": "Configuring Nginx as SSL Reverse Proxy. Basic configuration: Create server block in /etc/nginx/sites-available/. Listen on port 443 with SSL: 'listen 443 ssl http2; ssl_certificate /path/to/cert.pem; ssl_certificate_key /path/to/key.pem;'. Configure upstream backend: 'upstream backend { server localhost:8080; }'. Proxy requests: 'location / { proxy_pass http://backend; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; }'. Redirect HTTP to HTTPS: 'server { listen 80; return 301 https://$server_name$request_uri; }'. SSL best practices: Use strong ciphers: 'ssl_ciphers HIGH:!aNULL:!MD5;'. Enable HSTS: 'add_header Strict-Transport-Security \"max-age=31536000\";'. Use TLS 1.2+ only: 'ssl_protocols TLSv1.2 TLSv1.3;'. Get free certificates from Let's Encrypt using certbot. Test with 'nginx -t' before reloading. Monitor logs for errors."}, {"id": "doc_2", "text": "Nginx architecture and worker processes: Nginx uses an event-driven, asynchronous architecture that allows it to handle thousands of concurrent connections efficiently with low memory footprint."}, {"id": "doc_3", "text": "Web server comparison: Apache vs Nginx vs other servers. Each has different strengths - Apache excels at .htaccess flexibility, Nginx at static content and reverse proxy, while newer servers explore different architectures."}, {"id": "doc_4", "text": "SSL/TLS protocol fundamentals including handshake process, certificate chains, and cryptographic algorithms. Understanding these concepts helps troubleshoot certificate issues."}, {"id": "doc_5", "text": "Load balancing strategies include round-robin, least connections, IP hash, and weighted distribution. Choose based on your application's stateless/stateful nature and traffic patterns."}], "relevant_id": "doc_1"}
{"query": "docker compose environment variables", "candidates": [{"id": "doc_1", "text": "Docker Compose Environment Variables Complete Guide. Three ways to set environment variables: 1) In docker-compose.yml using 'environment:' key: 'environment: - DATABASE_URL=postgres://localhost - DEBUG=true'. 2) Using 'env_file:' to load from file: 'env_file: - .env'. 3) Variable substitution from shell: 'image: postgres:${POSTGRES_VERSION}' with .env file containing 'POSTGRES_VERSION=13'. Precedence order (highest to lowest): Compose file 'environment:', Shell environment, .env file, Dockerfile ENV. Access in containers normally through process.env or equivalent. Use .env for defaults, override with shell exports for deployment-specific values. Pass secrets securely: don't commit .env to git, use Docker secrets in swarm mode, or external secret management. Validate required variables in application startup. Use ${VAR:-default} for defaults in compose file. Debug with 'docker-compose config' to see resolved values. Reference syntax: $VAR or ${VAR}."}, {"id": "doc_2", "text": "Docker Compose networking allows containers to communicate. By default, Compose creates a network for your app where each service is reachable by its service name as hostname."}, {"id": "doc_3", "text": "Container orchestration with Docker Swarm and Kubernetes. Docker Compose is primarily for development and single-host deployments, while these tools handle production multi-host scenarios."}, {"id": "doc_4", "text": "Twelve-factor app methodology provides best practices for building modern applications including configuration management, dependency isolation, and environment parity."}, {"id": "doc_5", "text": "Docker volumes and bind mounts provide persistent storage for containers. Volumes are managed by Docker while bind mounts use host filesystem paths directly."}], "relevant_id": "doc_1"}
{"query": "python async await asyncio tutorial", "candidates": [{"id": "doc_1", "text": "Python Async/Await and Asyncio Tutorial. Async programming allows concurrent I/O operations without threading. Basic concepts: Define async functions with 'async def': 'async def fetch_data(): await asyncio.sleep(1)'. Use 'await' to wait for async operations - only inside async functions. Run async code: 'asyncio.run(main())' for Python 3.7+. Create tasks for concurrent execution: 'task1 = asyncio.create_task(fetch(url1)); task2 = asyncio.create_task(fetch(url2)); await task1; await task2'. Gather multiple tasks: 'results = await asyncio.gather(task1, task2, task3)'. Common patterns: Use aiohttp for async HTTP: 'async with aiohttp.ClientSession() as session: async with session.get(url) as response: data = await response.text()'. Handle exceptions with try/except around await. Use asyncio.Queue for producer-consumer patterns. Semaphores limit concurrency: 'sem = asyncio.Semaphore(10)'. Don't mix blocking and async code - use 'loop.run_in_executor()' for blocking operations. When to use: I/O-bound operations like network requests, database queries, file I/O. Not for CPU-bound tasks - use multiprocessing instead."}, {"id": "doc_2", "text": "Python concurrency models include threading, multiprocessing, and async I/O. Each model suits different use cases - threading for I/O with shared memory, multiprocessing for CPU-bound parallelism."}, {"id": "doc_3", "text": "Event loops and coroutines form the foundation of async programming across many languages. Understanding the event loop helps you reason about async program execution."}, {"id": "doc_4", "text": "Python generators and iterators provide lazy evaluation and memory-efficient iteration. The yield keyword creates generators that can be paused and resumed."}, {"id": "doc_5", "text": "Concurrent programming challenges include race conditions, deadlocks, and resource contention. Proper synchronization and understanding of threading primitives helps avoid these issues."}, {"id": "doc_6", "text": "Python standard library overview covers essential modules for file I/O, networking, data serialization, and system interaction. Familiarity with standard library reduces dependency on external packages."}], "relevant_id": "doc_1"}
{"query": "terraform state file corruption recovery", "candidates": [{"id": "doc_1", "text": "Recovering from Terraform State File Corruption. State file issues can be critical - always backup before operations. Recovery steps: 1) Check version control for previous state file version. 2) Restore from remote backend backup if using S3/Terraform Cloud. 3) Use 'terraform state pull' to get current state before modifications. 4) For partial corruption, manually edit state JSON (last resort). 5) Rebuild state with 'terraform import' for each resource. Recovery techniques: If state is completely lost, use 'terraform import' to rebuild: 'terraform import aws_instance.example i-1234567890'. Import each resource individually matching your configuration. For corrupted entries, use 'terraform state rm' to remove bad resources, then import fresh. Use 'terraform state list' to see current resources. 'terraform state show resource.name' displays resource details. Prevention: Enable versioning on S3 backend, use state locking with DynamoDB, run 'terraform plan' before apply, use workspaces for isolation. Never manually edit state while Terraform is running. Consider Terraform Cloud for automatic state backups."}, {"id": "doc_2", "text": "Infrastructure as Code principles and best practices. Terraform, CloudFormation, and Pulumi allow you to define infrastructure declaratively, enabling version control and reproducibility."}, {"id": "doc_3", "text": "Terraform modules promote reusability and organization. Create modules for common patterns like VPCs, databases, or application stacks to standardize infrastructure across teams."}, {"id": "doc_4", "text": "Remote state backends in Terraform include S3, Azure Blob Storage, Google Cloud Storage, and Terraform Cloud. Remote state enables team collaboration and state locking."}, {"id": "doc_5", "text": "Terraform workspace management allows multiple environments (dev, staging, prod) from the same configuration. Workspaces provide state isolation for different deployments."}], "relevant_id": "doc_1"}
{"query": "mongodb aggregation pipeline performance", "candidates": [{"id": "doc_1", "text": "Optimizing MongoDB Aggregation Pipeline Performance. Aggregation pipelines can be slow on large collections without optimization. Key strategies: 1) Use $match early to filter documents before other stages. 2) Use $project early to reduce document size. 3) Create indexes for $match and $sort stages. 4) Use $limit after $sort to reduce processing. 5) Avoid $lookup when possible - denormalize data for read-heavy workloads. 6) Use $facet carefully as it runs multiple sub-pipelines. Stage order matters: filter first, transform later. Pipeline optimization tips: Check query plan with 'explain()' method. Use '$match' before '$unwind' to reduce documents processed. Index all fields used in $match, $sort, and $group. Consider $merge or $out for materializing results. Use allowDiskUse for large datasets that exceed memory. Monitor with profiler: 'db.setProfilingLevel(2)'. Use compound indexes for multiple filter fields. Avoid deeply nested $lookup operations. Consider map-reduce alternative for complex scenarios. Test pipeline stages incrementally. Use $sample for development testing."}, {"id": "doc_2", "text": "MongoDB schema design patterns for different use cases. Document databases allow flexible schemas, but proper design considering access patterns is crucial for performance."}, {"id": "doc_3", "text": "NoSQL database types include document stores, key-value stores, column-family stores, and graph databases. Each type excels at different data models and access patterns."}, {"id": "doc_4", "text": "MongoDB replication and sharding for scalability. Replica sets provide high availability while sharding distributes data across multiple servers for horizontal scaling."}, {"id": "doc_5", "text": "Database indexing strategies across relational and NoSQL databases. Understanding B-trees, hash indexes, and specialized indexes helps optimize query performance."}, {"id": "doc_6", "text": "CRUD operations in MongoDB use methods like insertOne, find, updateMany, and deleteOne. Understanding query operators and update modifiers is essential for effective database operations."}], "relevant_id": "doc_1"}
{"query": "kubernetes