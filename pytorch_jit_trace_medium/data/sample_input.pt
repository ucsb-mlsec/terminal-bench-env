import torch
import torch.nn as nn

# Define the model architecture
class ImageClassificationModel(nn.Module):
    def __init__(self):
        super(ImageClassificationModel, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * 28 * 28, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 10)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# Load the original model
model = ImageClassificationModel()
model.load_state_dict(torch.load('/workspace/model.pth'))
model.eval()

# Load sample input
sample_input = torch.load('/workspace/sample_input.pt')

# Get original model output for verification
with torch.no_grad():
    original_output = model(sample_input)

# Optimize the model using TorchScript with tracing
# This compiles the model into an optimized format that:
# 1. Loads faster (no Python overhead)
# 2. Executes faster (operator fusion, optimized kernel selection)
# 3. Maintains identical behavior
with torch.no_grad():
    traced_model = torch.jit.trace(model, sample_input)
    
    # Optimize the traced model
    traced_model = torch.jit.optimize_for_inference(traced_model)
    
    # Verify the optimized model produces identical outputs
    optimized_output = traced_model(sample_input)
    
    # Check that outputs match within tolerance
    max_diff = torch.max(torch.abs(original_output - optimized_output)).item()
    assert max_diff < 1e-5, f"Output mismatch: {max_diff}"
    
    # Save the optimized model
    torch.jit.save(traced_model, '/workspace/optimized_model.pt')

print("Optimized model saved successfully to /workspace/optimized_model.pt")
print(f"Maximum output difference: {max_diff:.2e}")