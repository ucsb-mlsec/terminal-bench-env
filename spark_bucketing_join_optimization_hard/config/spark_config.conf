# Production Spark Cluster Configuration
# Environment: 16-core executors with limited memory
# Issue: Shuffle operations causing disk spills during peak hours
# Current state: No bucketing configured, all joins use shuffle operations

# Executor Configuration
spark.executor.cores=16
spark.executor.memory=8g
spark.executor.instances=4

# Parallelism Settings
# Default parallelism based on total cores (16 cores * 4 executors = 64)
spark.default.parallelism=64

# Shuffle partitions currently set higher than optimal
# Causing small partition overhead and inefficient resource utilization
spark.sql.shuffle.partitions=200

# Broadcast Join Threshold (10MB)
# Small tables under this size will be broadcast instead of shuffled
spark.sql.autoBroadcastJoinThreshold=10485760

# Adaptive Query Execution
# Enabled to help with dynamic optimization, but doesn't solve shuffle issues
spark.sql.adaptive.enabled=true

# Memory Management
# 60% of heap for execution and storage
spark.memory.fraction=0.6

# 50% of spark.memory.fraction reserved for caching
# Rest available for execution (shuffle operations)
spark.memory.storageFraction=0.5

# Bucketing Support
# Bucketing is enabled but not currently configured on any tables
# This allows bucketed tables to be used when properly configured
spark.sql.sources.bucketing.enabled=true

# Auto Bucketed Scan
# When enabled, Spark will automatically use bucketing information
# to eliminate shuffle operations in joins when bucket specs match
spark.sql.sources.bucketing.autoBucketedScan.enabled=true

# Performance Observations:
# - Customer profile: 10M+ records with customer_id primary key
# - Transactions: 100M+ records with customer_id foreign key
# - Join operations cause extensive shuffling (no bucketing configured)
# - Memory pressure during shuffles causing spills to disk
# - SLA violations during peak processing hours
# - Target: 40%+ performance improvement needed