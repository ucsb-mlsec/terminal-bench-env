I'll help you solve this task. Let me start by examining the schema and then processing the binary files.

First, let me check the schema file and the logs directory structure, then create a solution to deserialize the Thrift binary files and extract the required metrics.

```python
import struct
import json
import os
from pathlib import Path

# Thrift binary protocol field type IDs
THRIFT_TYPE_STOP = 0
THRIFT_TYPE_BOOL = 2
THRIFT_TYPE_I08 = 3
THRIFT_TYPE_I16 = 6
THRIFT_TYPE_I32 = 8
THRIFT_TYPE_I64 = 10
THRIFT_TYPE_DOUBLE = 4
THRIFT_TYPE_STRING = 11
THRIFT_TYPE_STRUCT = 12

class ThriftBinaryReader:
    def __init__(self, data):
        self.data = data
        self.pos = 0
    
    def read_byte(self):
        val = struct.unpack('!b', self.data[self.pos:self.pos+1])[0]
        self.pos += 1
        return val
    
    def read_i16(self):
        val = struct.unpack('!h', self.data[self.pos:self.pos+2])[0]
        self.pos += 2
        return val
    
    def read_i32(self):
        val = struct.unpack('!i', self.data[self.pos:self.pos+4])[0]
        self.pos += 4
        return val
    
    def read_i64(self):
        val = struct.unpack('!q', self.data[self.pos:self.pos+8])[0]
        self.pos += 8
        return val
    
    def read_double(self):
        val = struct.unpack('!d', self.data[self.pos:self.pos+8])[0]
        self.pos += 8
        return val
    
    def read_string(self):
        length = self.read_i32()
        val = self.data[self.pos:self.pos+length].decode('utf-8')
        self.pos += length
        return val
    
    def read_field_header(self):
        field_type = self.read_byte()
        if field_type == THRIFT_TYPE_STOP:
            return field_type, 0
        field_id = self.read_i16()
        return field_type, field_id

def deserialize_server_metrics(binary_data):
    """Deserialize a ServerMetrics struct from Thrift binary format"""
    reader = ThriftBinaryReader(binary_data)
    metrics = {}
    
    while True:
        field_type, field_id = reader.read_field_header()
        
        if field_type == THRIFT_TYPE_STOP:
            break
        
        if field_id == 1:  # server_id: i32
            metrics['server_id'] = reader.read_i32()
        elif field_id == 2:  # hostname: string
            metrics['hostname'] = reader.read_string()
        elif field_id == 3:  # timestamp: i64
            metrics['timestamp'] = reader.read_i64()
        elif field_id == 4:  # cpu_percent: double
            metrics['cpu_percent'] = reader.read_double()
        elif field_id == 5:  # memory_bytes: i64
            metrics['memory_bytes'] = reader.read_i64()
        elif field_id == 6:  # error_count: i32
            metrics['error_count'] = reader.read_i32()
        elif field_id == 7:  # status: string
            metrics['status'] = reader.read_string()
        else:
            # Skip unknown field
            if field_type == THRIFT_TYPE_I32:
                reader.read_i32()
            elif field_type == THRIFT_TYPE_I64:
                reader.read_i64()
            elif field_type == THRIFT_TYPE_DOUBLE:
                reader.read_double()
            elif field_type == THRIFT_TYPE_STRING:
                reader.read_string()
    
    return metrics

def process_logs():
    logs_dir = Path('/workspace/logs')
    
    servers_processed = 0
    peak_cpu_percent = 0.0
    total_errors = 0
    
    # Process all .bin files
    for bin_file in sorted(logs_dir.glob('*.bin')):
        try:
            with open(bin_file, 'rb') as f:
                binary_data = f.read()
            
            metrics = deserialize_server_metrics(binary_data)
            
            servers_processed += 1
            
            # Track peak CPU
            if 'cpu_percent' in metrics:
                peak_cpu_percent = max(peak_cpu_percent, metrics['cpu_percent'])
            
            # Sum errors
            if 'error_count' in metrics:
                total_errors += metrics['error_count']
                
        except Exception as e:
            print(f"Error processing {bin_file}: {e}")
            continue
    
    # Save results
    results = {
        "servers_processed": servers_processed,
        "peak_cpu_percent": peak_cpu_percent,
        "total_errors": total_errors
    }
    
    with open('/workspace/summary.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    return results

# Run the processing
if __name__ == '__main__':
    results = process_logs()
    print(json.dumps(results, indent=2))