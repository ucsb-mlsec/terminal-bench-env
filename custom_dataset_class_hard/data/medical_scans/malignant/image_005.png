#!/usr/bin/env python3
"""
Medical Scan Dataset Implementation for PyTorch
"""

import os
import json
from pathlib import Path
from typing import Tuple, List
import torch
from torch.utils.data import Dataset
from PIL import Image
import numpy as np


class MedicalScanDataset(Dataset):
    """
    Custom PyTorch Dataset for medical scan images.
    
    Handles loading, preprocessing, and validation of medical scan images
    organized in subdirectories by diagnosis category.
    """
    
    def __init__(self, root_dir: str):
        """
        Initialize the dataset.
        
        Args:
            root_dir: Root directory containing subdirectories with images
        """
        self.root_dir = Path(root_dir)
        self.target_size = (256, 256)
        self.categories = ['benign', 'malignant', 'uncertain']
        self.label_map = {cat: idx for idx, cat in enumerate(self.categories)}
        
        # Load all valid images
        self.samples = []
        self._load_samples()
    
    def _load_samples(self):
        """
        Scan directory structure and collect all valid image paths.
        Skips corrupted files that cannot be loaded.
        """
        for category in self.categories:
            category_path = self.root_dir / category
            
            if not category_path.exists():
                continue
            
            label = self.label_map[category]
            
            # Find all image files
            for img_path in category_path.glob('*.png'):
                # Try to load image to validate it's not corrupted
                try:
                    with Image.open(img_path) as img:
                        # Verify image can be loaded
                        img.verify()
                    
                    # Re-open for actual loading (verify closes the file)
                    with Image.open(img_path) as img:
                        # Test that we can actually load the data
                        _ = np.array(img)
                    
                    # If successful, add to samples
                    self.samples.append((str(img_path), label))
                    
                except Exception as e:
                    # Skip corrupted files
                    print(f"Skipping corrupted file {img_path}: {e}")
                    continue
            
            # Also check for jpg files
            for img_path in category_path.glob('*.jpg'):
                try:
                    with Image.open(img_path) as img:
                        img.verify()
                    
                    with Image.open(img_path) as img:
                        _ = np.array(img)
                    
                    self.samples.append((str(img_path), label))
                    
                except Exception:
                    continue
    
    def __len__(self) -> int:
        """Return the total number of valid samples."""
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        """
        Load and preprocess a single sample.
        
        Args:
            idx: Index of the sample to load
            
        Returns:
            Tuple of (image_tensor, label) where image_tensor has shape (1, 256, 256)
        """
        img_path, label = self.samples[idx]
        
        # Load image
        with Image.open(img_path) as img:
            # Convert to grayscale if needed
            if img.mode != 'L':
                img = img.convert('L')
            
            # Resize to target size
            img = img.resize(self.target_size, Image.BILINEAR)
            
            # Convert to numpy array
            img_array = np.array(img, dtype=np.float32)
            
            # Normalize to [0, 1] range
            # Handle both 0-255 and 0-1 ranges
            if img_array.max() > 1.0:
                img_array = img_array / 255.0
            
            # Convert to tensor with shape (1, H, W)
            img_tensor = torch.from_numpy(img_array).unsqueeze(0)
        
        return img_tensor, label
    
    def get_class_distribution(self) -> List[int]:
        """
        Calculate the distribution of samples across classes.
        
        Returns:
            List of counts [benign_count, malignant_count, uncertain_count]
        """
        distribution = [0, 0, 0]
        for _, label in self.samples:
            distribution[label] += 1
        return distribution


def create_validation_summary(dataset: MedicalScanDataset, output_path: str):
    """
    Create a validation summary JSON file with dataset statistics.
    
    Args:
        dataset: The MedicalScanDataset instance to validate
        output_path: Path where the JSON summary should be saved
    """
    # Get total valid images
    total_valid_images = len(dataset)
    
    # Get class distribution
    class_distribution = dataset.get_class_distribution()
    
    # Get image shape from first sample
    if total_valid_images > 0:
        first_image, _ = dataset[0]
        image_shape = list(first_image.shape)
        sample_pixel_mean = round(float(first_image.mean()), 3)
    else:
        image_shape = [1, 256, 256]
        sample_pixel_mean = 0.0
    
    # Create summary dictionary
    summary = {
        "total_valid_images": total_valid_images,
        "class_distribution": class_distribution,
        "image_shape": image_shape,
        "sample_pixel_mean": sample_pixel_mean
    }
    
    # Save to JSON file
    with open(output_path, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"Validation summary saved to {output_path}")
    print(f"Total valid images: {total_valid_images}")
    print(f"Class distribution: {class_distribution}")
    print(f"Image shape: {image_shape}")
    print(f"Sample pixel mean: {sample_pixel_mean}")


if __name__ == "__main__":
    # Create dataset instance
    dataset_path = "/workspace/medical_scans"
    dataset = MedicalScanDataset(dataset_path)
    
    # Create validation summary
    summary_path = "/workspace/validation_summary.json"
    create_validation_summary(dataset, summary_path)