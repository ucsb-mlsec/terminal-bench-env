import os
import json
import torch
from torch.utils.data import Dataset
from PIL import Image
import numpy as np
from pathlib import Path


class MedicalScanDataset(Dataset):
    """
    Custom PyTorch Dataset for loading medical scan images.
    
    Handles:
    - Multiple diagnosis categories (benign, malignant, uncertain)
    - Inconsistent image dimensions
    - Corrupted files
    - Pixel value normalization
    """
    
    def __init__(self, root_dir):
        """
        Initialize the dataset by scanning the directory structure.
        
        Args:
            root_dir (str): Root directory containing subdirectories for each class
        """
        self.root_dir = Path(root_dir)
        self.class_to_idx = {'benign': 0, 'malignant': 1, 'uncertain': 2}
        self.target_size = (256, 256)
        
        # Store valid image paths and labels
        self.samples = []
        self.class_counts = [0, 0, 0]
        
        self._load_dataset()
    
    def _load_dataset(self):
        """
        Scan directory structure and collect all valid image paths.
        Corrupted files are skipped.
        """
        for class_name, class_idx in self.class_to_idx.items():
            class_dir = self.root_dir / class_name
            
            if not class_dir.exists():
                continue
            
            # Get all image files in the class directory
            image_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']
            
            for img_path in class_dir.iterdir():
                if img_path.suffix.lower() in image_extensions:
                    # Try to validate the image can be loaded
                    if self._validate_image(img_path):
                        self.samples.append((str(img_path), class_idx))
                        self.class_counts[class_idx] += 1
    
    def _validate_image(self, img_path):
        """
        Validate that an image can be loaded without errors.
        
        Args:
            img_path: Path to the image file
            
        Returns:
            bool: True if image is valid, False otherwise
        """
        try:
            with Image.open(img_path) as img:
                img.verify()
            return True
        except Exception:
            return False
    
    def _load_and_preprocess(self, img_path):
        """
        Load and preprocess an image.
        
        Args:
            img_path: Path to the image file
            
        Returns:
            torch.Tensor: Preprocessed image tensor of shape (1, 256, 256)
        """
        # Load image
        img = Image.open(img_path)
        
        # Convert to grayscale if needed
        if img.mode != 'L':
            img = img.convert('L')
        
        # Resize to target size
        img = img.resize(self.target_size, Image.BILINEAR)
        
        # Convert to numpy array
        img_array = np.array(img, dtype=np.float32)
        
        # Normalize to [0, 1] range
        if img_array.max() > 1.0:
            img_array = img_array / 255.0
        
        # Convert to tensor and add channel dimension
        img_tensor = torch.from_numpy(img_array).unsqueeze(0)
        
        return img_tensor
    
    def __len__(self):
        """Return the total number of valid samples."""
        return len(self.samples)
    
    def __getitem__(self, idx):
        """
        Get a sample from the dataset.
        
        Args:
            idx: Index of the sample
            
        Returns:
            tuple: (image_tensor, label) where image_tensor has shape (1, 256, 256)
        """
        img_path, label = self.samples[idx]
        
        try:
            img_tensor = self._load_and_preprocess(img_path)
            return img_tensor, label
        except Exception as e:
            # If loading fails at runtime, return a zero tensor
            # This is a fallback that shouldn't normally happen due to validation
            print(f"Warning: Failed to load {img_path}: {e}")
            return torch.zeros(1, 256, 256), label
    
    def get_class_distribution(self):
        """
        Get the distribution of samples across classes.
        
        Returns:
            list: [benign_count, malignant_count, uncertain_count]
        """
        return self.class_counts.copy()


def create_validation_summary(dataset, output_path):
    """
    Create a validation summary JSON file.
    
    Args:
        dataset: MedicalScanDataset instance
        output_path: Path to save the JSON file
    """
    # Get total valid images
    total_valid_images = len(dataset)
    
    # Get class distribution
    class_distribution = dataset.get_class_distribution()
    
    # Get image shape from first sample
    if total_valid_images > 0:
        first_image, _ = dataset[0]
        image_shape = list(first_image.shape)
        sample_pixel_mean = round(float(first_image.mean()), 3)
    else:
        image_shape = [1, 256, 256]
        sample_pixel_mean = 0.0
    
    # Create summary dictionary
    summary = {
        "total_valid_images": total_valid_images,
        "class_distribution": class_distribution,
        "image_shape": image_shape,
        "sample_pixel_mean": sample_pixel_mean
    }
    
    # Save to JSON
    with open(output_path, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"Validation summary saved to {output_path}")
    print(f"Total valid images: {total_valid_images}")
    print(f"Class distribution: {class_distribution}")
    print(f"Image shape: {image_shape}")
    print(f"Sample pixel mean: {sample_pixel_mean}")


if __name__ == "__main__":
    # Create dataset instance
    dataset = MedicalScanDataset("/workspace/medical_scans")
    
    # Create validation summary
    create_validation_summary(dataset, "/workspace/validation_summary.json")