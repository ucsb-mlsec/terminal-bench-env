Sample Sensor Data - Illustrating Scale and Distribution

================================================================================
SYSTEM OVERVIEW STATISTICS
================================================================================

Total Sensors: 15,247
Data Centers: 5
  - DC-EAST: 3,241 sensors
  - DC-WEST: 3,105 sensors
  - DC-CENTRAL: 2,987 sensors
  - DC-NORTH: 3,012 sensors
  - DC-SOUTH: 2,902 sensors

Reading Types: temperature, humidity, pressure
Write Frequency: Every 30 seconds per sensor
Daily Writes Per Sensor: 2,880 readings (86,400 seconds ÷ 30 seconds)
Total Daily Writes: ~43,911,360 readings (15,247 sensors × 2,880 readings)
90-Day Data Volume: ~3,952,022,400 readings
Average Writes Per Second: ~508 writes/sec (peak can reach 50,000/sec during bursts)

================================================================================
SAMPLE DATA RECORDS
================================================================================

sensor_id     | timestamp           | data_center | reading_type | value
--------------|---------------------|-------------|--------------|----------
SENSOR_00001  | 2024-01-15 14:23:30 | DC-EAST     | temperature  | 23.4
SENSOR_00001  | 2024-01-15 14:23:30 | DC-EAST     | humidity     | 45.2
SENSOR_00001  | 2024-01-15 14:23:30 | DC-EAST     | pressure     | 1013.25
SENSOR_00001  | 2024-01-15 14:23:00 | DC-EAST     | temperature  | 23.5
SENSOR_00001  | 2024-01-15 14:23:00 | DC-EAST     | humidity     | 45.1
SENSOR_00001  | 2024-01-15 14:23:00 | DC-EAST     | pressure     | 1013.20
SENSOR_00542  | 2024-01-15 14:23:45 | DC-EAST     | temperature  | 21.8
SENSOR_00542  | 2024-01-15 14:23:45 | DC-EAST     | humidity     | 52.3
SENSOR_00542  | 2024-01-15 14:23:45 | DC-EAST     | pressure     | 1012.95
SENSOR_03500  | 2024-01-15 14:24:00 | DC-WEST     | temperature  | 24.1
SENSOR_03500  | 2024-01-15 14:24:00 | DC-WEST     | humidity     | 38.7
SENSOR_03500  | 2024-01-15 14:24:00 | DC-WEST     | pressure     | 1014.10
SENSOR_06700  | 2024-01-15 14:24:15 | DC-CENTRAL  | temperature  | 22.3
SENSOR_06700  | 2024-01-15 14:24:15 | DC-CENTRAL  | humidity     | 48.9
SENSOR_06700  | 2024-01-15 14:24:15 | DC-CENTRAL  | pressure     | 1013.75
SENSOR_09800  | 2024-01-15 14:24:30 | DC-NORTH    | temperature  | 19.5
SENSOR_09800  | 2024-01-15 14:24:30 | DC-NORTH    | humidity     | 61.2
SENSOR_09800  | 2024-01-15 14:24:30 | DC-NORTH    | pressure     | 1015.30
SENSOR_13200  | 2024-01-15 14:24:45 | DC-SOUTH    | temperature  | 26.8
SENSOR_13200  | 2024-01-15 14:24:45 | DC-SOUTH    | humidity     | 42.5
SENSOR_13200  | 2024-01-15 14:24:45 | DC-SOUTH    | pressure     | 1011.85
SENSOR_00001  | 2024-01-14 08:15:00 | DC-EAST     | temperature  | 22.1
SENSOR_00001  | 2024-01-13 16:42:30 | DC-EAST     | temperature  | 24.6
SENSOR_00542  | 2024-01-12 22:10:15 | DC-EAST     | humidity     | 49.8
SENSOR_03500  | 2024-01-11 11:33:45 | DC-WEST     | pressure     | 1013.50

================================================================================
PROBLEM STATEMENT - CURRENT SCHEMA ISSUES
================================================================================

ISSUE #1: HOT PARTITIONS
The current partition key causes all sensors in a single data center to write 
to the same partition, creating severe hot spots.

Example - DC-EAST Partition:
  - Number of sensors: 3,241
  - Writes per sensor per day: 2,880
  - Reading types per write: 3 (temperature, humidity, pressure)
  - Daily writes to ONE partition: 3,241 × 2,880 × 3 = 28,002,240 writes/day
  - This single partition receives ~324 writes/second continuously
  - During peak loads, this partition can receive 10,000+ writes/second
  
ISSUE #2: UNBOUNDED PARTITION GROWTH
With 90-day retention requirement:
  - DC-EAST partition over 90 days: ~2,520,201,600 rows in ONE partition
  - Partition size: ~2.5 BILLION rows (far exceeds Cassandra recommendations)
  - Recommended max partition size: ~100,000 rows or 100MB
  - Current partition size: Multiple GB per partition (100x+ over limit)
  
ISSUE #3: QUERY PERFORMANCE DEGRADATION
Large partitions cause:
  - Slow single-sensor queries (must scan billions of rows)
  - Memory pressure during reads
  - Compaction storms
  - Node instability and crashes
  - Query timeouts exceeding 100ms SLA by 10-50x

================================================================================
QUERY PATTERN EXAMPLES
================================================================================

QUERY 1 - Single Sensor Time Range (most common - 85% of queries):
  SELECT * FROM sensor_readings 
  WHERE sensor_id = 'SENSOR_00001' 
    AND timestamp >= '2024-01-14 00:00:00' 
    AND timestamp < '2024-01-15 00:00:00'
  
  Problem: With current schema using data_center as partition key, this query
  must scan the entire DC-EAST partition containing 3,241 sensors worth of data.

QUERY 2 - Latest Reading for All Sensors in Data Center (10% of queries):
  SELECT * FROM sensor_readings 
  WHERE data_center = 'DC-EAST' 
    AND timestamp > '2024-01-15 14:20:00'
  
  Problem: Hits hot partition with billions of rows.

QUERY 3 - Single Sensor Latest Reading (5% of queries):
  SELECT * FROM sensor_readings 
  WHERE sensor_id = 'SENSOR_00001' 
  ORDER BY timestamp DESC 
  LIMIT 1
  
  Problem: Requires scanning large partition to find sensor data.

================================================================================
ROOT CAUSE ANALYSIS
================================================================================

The current schema uses data_center as the partition key:
  PRIMARY KEY ((data_center), sensor_id, timestamp)

This design creates TWO critical failures:

1. POOR DATA DISTRIBUTION: Only 5 partitions for 15,000+ sensors
   - All writes to DC-EAST go to one partition (hot spot)
   - Uneven load distribution across cluster nodes
   - No ability to scale horizontally

2. UNBOUNDED GROWTH: Partitions grow infinitely over time
   - Each partition accumulates 90 days of data for thousands of sensors
   - No natural boundary to limit partition size
   - Violates Cassandra best practice of bounded partitions

REQUIRED SOLUTION:
- Partition key must distribute writes evenly (avoid hot partitions)
- Partition key must limit partition size (prevent unbounded growth)
- Must support efficient single-sensor queries by time range
- Must maintain timestamp ordering within partitions